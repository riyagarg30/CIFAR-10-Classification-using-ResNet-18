{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hSnP03Pkyr0b"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import glob\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "import pickle\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "_NRtb40Ue-f-",
    "outputId": "d7c8ec65-3f47-4875-d79a-e08d4254c22b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/pb3073/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpb3073\u001b[0m (\u001b[33mpb3073-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/pb3073/notebooks/wandb/run-20250313_180158-8pm6w0c0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pb3073-new-york-university/DL_project1/runs/8pm6w0c0' target=\"_blank\">resnet_experiment_20250313_180158</a></strong> to <a href='https://wandb.ai/pb3073-new-york-university/DL_project1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pb3073-new-york-university/DL_project1' target=\"_blank\">https://wandb.ai/pb3073-new-york-university/DL_project1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pb3073-new-york-university/DL_project1/runs/8pm6w0c0' target=\"_blank\">https://wandb.ai/pb3073-new-york-university/DL_project1/runs/8pm6w0c0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# key: ed118af0347d47519a2b9d68527b0032bcf73907\n",
    "wandb.login(key='ed118af0347d47519a2b9d68527b0032bcf73907')\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Define parameters\n",
    "name = f\"resnet_experiment_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}\"\n",
    "num_epochs = 300\n",
    "warmup_epochs = 10          # number of epochs to warm up the learning rate\n",
    "batch_size = 64\n",
    "has_checkpoint = False\n",
    "learning_rate = 0.02  # initial learning rate\n",
    "augmentation_ratio = 0.6  # percentage of data to be augmented (60%)\n",
    "train_val_ratio = 0.8  # percentage of data to be considered for training\n",
    "block_size = [3, 3, 4, 3]\n",
    "dropout_prob = 0.3\n",
    "use_se = True\n",
    "se_reduction = 8 # dictates how much the channel dimension is compressed. The lower it is, the lesser the compression and more nuances learnt\n",
    "MIXUP_ALPHA = 0.3         # Mixup Beta distribution parameter\n",
    "CUTMIX_ALPHA = 1.0        # CutMix Beta distribution parameter\n",
    "ADVANCED_MIXUP_PROB = 0.5   # Probability of applying CutMix vs. Mixup\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "\n",
    "\n",
    "# Dynamically populate the architecture description string\n",
    "architecture_description = (\n",
    "    f\"ResNet with layers {block_size}, \"\n",
    "    f\"warmup {warmup_epochs} epochs, \"\n",
    "    f\"total {num_epochs} epochs, \"\n",
    "    f\"{int(augmentation_ratio * 100)}%-{int((1 - augmentation_ratio) * 100)}% augmentation split, \"\n",
    "    f\"{int(train_val_ratio * 100)}-{int((1 - train_val_ratio) * 100)} train-val split, \"\n",
    "    f\"batch size {batch_size}, \"\n",
    "    f\"dropout {dropout_prob}, \"\n",
    "    f\"Squeeze-and-Excite {'enabled' if use_se else 'disabled'} (reduction={se_reduction}), \"\n",
    "    f\"momentum {momentum}, \"\n",
    "    f\"weight decay {weight_decay}\"\n",
    "    f\"MIXUP_ALPHA {MIXUP_ALPHA}\"\n",
    "    f\"CUTMIX_ALPHA {CUTMIX_ALPHA}\"\n",
    "    \"implemented cutmix and normal mixup, squeeze and excite\"\n",
    ")\n",
    "\n",
    "# Initialize wandb with the dynamic config\n",
    "wandb.init(\n",
    "    project=\"DL_project1\",\n",
    "    name=name,\n",
    "    config={\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"warmup_epochs\": warmup_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"augmentation_ratio\": augmentation_ratio,\n",
    "        \"train_val_ratio\": train_val_ratio,\n",
    "        \"block_size\": block_size,\n",
    "        \"dropout_prob\": dropout_prob,\n",
    "        \"use_se\": use_se,\n",
    "        \"se_reduction\": se_reduction,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"momentum\": momentum,\n",
    "        \"architecture\": architecture_description,\n",
    "        \"dataset\": \"CIFAR10\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiR4Tf2yO7in",
    "outputId": "52c2f78b-c497-4c01-e995-31b6b2d733cd"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Defining Transformers for train and test set differently\n",
    "# Load the dataset with training transforms\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomRotation(5),\n",
    "#     transforms.RandomHorizontalFlip(0.5),\n",
    "#     transforms.RandomCrop(32, padding=2),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "#                          std=[0.2023, 0.1994, 0.2010])\n",
    "# ])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    #transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.1, 0.2), ratio=(0.5, 2.0), value=\"random\"),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# Download the full training set (will be split into train and validation)\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "\n",
    "# Determine sizes for training and validation splits\n",
    "validation_split = 0.15\n",
    "val_size = int(len(full_train_dataset) * validation_split)\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Define test dataset separately\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M68-fE0xvD76",
    "outputId": "aefbb300-6b99-4e25-ed31-047efdc8284c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9a3UAmMvXJN",
    "outputId": "6b300164-9c3a-446d-99d9-d03498a60996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x151902399e20>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2V5MRrtnfPGQ"
   },
   "outputs": [],
   "source": [
    "class SqueezeExcite(nn.Module):\n",
    "    def __init__(self, num_channels, reduction=8):\n",
    "        \"\"\"\n",
    "        Squeeze-and-Excite block.\n",
    "\n",
    "        Args:\n",
    "            num_channels (int): Number of input channels.\n",
    "            reduction (int): Reduction ratio for the bottleneck. Default: 8.\n",
    "        \"\"\"\n",
    "        super(SqueezeExcite, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(num_channels, num_channels // reduction, kernel_size=1, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(num_channels // reduction, num_channels, kernel_size=1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Squeeze: Global average pooling\n",
    "        se = self.global_avg_pool(x)\n",
    "        # Excitation: two FC layers with ReLU and Sigmoid activations\n",
    "        se = self.fc1(se)\n",
    "        se = self.relu(se)\n",
    "        se = self.fc2(se)\n",
    "        se = self.sigmoid(se)\n",
    "        # Scale: multiply the original input with the learned channel weights\n",
    "        return x * se\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None,\n",
    "                 use_se=False, se_reduction=8):\n",
    "        \"\"\"\n",
    "        Residual block with an optional Squeeze-and-Excite module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            stride (int): Stride for the first convolution. Default: 1.\n",
    "            downsample (nn.Module or None): Downsampling layer if needed.\n",
    "            use_se (bool): Whether to include the SE block. Default: False.\n",
    "            se_reduction (int): Reduction ratio for the SE block. Default: 8.\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.use_se = use_se\n",
    "        if self.use_se:\n",
    "            self.se = SqueezeExcite(out_channels, reduction=se_reduction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)  # First activation\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Optionally apply Squeeze-and-Excite\n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)  # Second activation\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uM26Uaj5YAFy"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNetWithDropout(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10, dropout_prob=0.5, use_se=False, se_reduction=8):\n",
    "        \"\"\"\n",
    "        ResNet with dropout and optional SE blocks.\n",
    "\n",
    "        Args:\n",
    "            block: Residual block type (e.g., ResidualBlock).\n",
    "            layers (list): Number of blocks in each layer.\n",
    "            num_classes (int): Number of output classes.\n",
    "            dropout_prob (float): Dropout probability before the final FC layer.\n",
    "            use_se (bool): Whether to include SE blocks in each residual block.\n",
    "            se_reduction (int): Reduction ratio for SE blocks.\n",
    "        \"\"\"\n",
    "        super(ResNetWithDropout, self).__init__()\n",
    "        self.use_se = use_se\n",
    "        self.se_reduction = se_reduction\n",
    "        self.inplanes = 32\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer1 = self._make_layer(block, 32, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        # Pass the external SE configuration to the block\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample,\n",
    "                            use_se=self.use_se, se_reduction=self.se_reduction))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, use_se=self.use_se, se_reduction=self.se_reduction))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)  # Apply dropout before the final FC layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example usage with label smoothing in the loss function:\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        logprobs = F.log_softmax(pred, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Kaiming normal initialization for Conv2d layers\n",
    "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # Kaiming normal initialization for Linear layers\n",
    "        init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        # Initialize BatchNorm weights and biases\n",
    "        init.constant_(m.weight, 1)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Instantiate the improved model\n",
    "model = ResNetWithDropout(ResidualBlock, block_size, num_classes=10, dropout_prob=dropout_prob, use_se=use_se, se_reduction=se_reduction).to(device)\n",
    "# He Initialization\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rjDDgF4NfUzQ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Instantiate the model\n",
    "# 3, 3, 6, 3\n",
    "# model = ResNet(ResidualBlock, [2, 2, 2, 2]).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # [CHANGED] Original optimizer using SGD with momentum\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.001, momentum=0.9)\n",
    "\n",
    "# -------------------------------\n",
    "# [CHANGED] Warmup and Cosine Annealing Scheduler Setup\n",
    "# -------------------------------\n",
    "total_epochs = num_epochs  # total number of epochs in training\n",
    "start_epoch = 0\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    # Warmup phase: linearly increase LR during the first 'warmup_epochs'\n",
    "    if epoch < warmup_epochs:\n",
    "        return float(epoch + 1) / warmup_epochs\n",
    "    # Cosine annealing after warmup\n",
    "    else:\n",
    "        # Adjust epoch number for cosine annealing starting at 0 after warmup\n",
    "        cosine_epoch = epoch - warmup_epochs\n",
    "        cosine_total = total_epochs - warmup_epochs\n",
    "        return 0.5 * (1 + math.cos(math.pi * cosine_epoch / cosine_total))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcCgiHUO0HWK",
    "outputId": "91468f1b-46a7-44c0-fd8a-ec7dbfda6dc7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             864\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "              ReLU-3           [-1, 32, 32, 32]               0\n",
      "            Conv2d-4           [-1, 32, 32, 32]           9,216\n",
      "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
      "              ReLU-6           [-1, 32, 32, 32]               0\n",
      "            Conv2d-7           [-1, 32, 32, 32]           9,216\n",
      "       BatchNorm2d-8           [-1, 32, 32, 32]              64\n",
      " AdaptiveAvgPool2d-9             [-1, 32, 1, 1]               0\n",
      "           Conv2d-10              [-1, 4, 1, 1]             132\n",
      "             ReLU-11              [-1, 4, 1, 1]               0\n",
      "           Conv2d-12             [-1, 32, 1, 1]             160\n",
      "          Sigmoid-13             [-1, 32, 1, 1]               0\n",
      "    SqueezeExcite-14           [-1, 32, 32, 32]               0\n",
      "             ReLU-15           [-1, 32, 32, 32]               0\n",
      "    ResidualBlock-16           [-1, 32, 32, 32]               0\n",
      "           Conv2d-17           [-1, 32, 32, 32]           9,216\n",
      "      BatchNorm2d-18           [-1, 32, 32, 32]              64\n",
      "             ReLU-19           [-1, 32, 32, 32]               0\n",
      "           Conv2d-20           [-1, 32, 32, 32]           9,216\n",
      "      BatchNorm2d-21           [-1, 32, 32, 32]              64\n",
      "AdaptiveAvgPool2d-22             [-1, 32, 1, 1]               0\n",
      "           Conv2d-23              [-1, 4, 1, 1]             132\n",
      "             ReLU-24              [-1, 4, 1, 1]               0\n",
      "           Conv2d-25             [-1, 32, 1, 1]             160\n",
      "          Sigmoid-26             [-1, 32, 1, 1]               0\n",
      "    SqueezeExcite-27           [-1, 32, 32, 32]               0\n",
      "             ReLU-28           [-1, 32, 32, 32]               0\n",
      "    ResidualBlock-29           [-1, 32, 32, 32]               0\n",
      "           Conv2d-30           [-1, 32, 32, 32]           9,216\n",
      "      BatchNorm2d-31           [-1, 32, 32, 32]              64\n",
      "             ReLU-32           [-1, 32, 32, 32]               0\n",
      "           Conv2d-33           [-1, 32, 32, 32]           9,216\n",
      "      BatchNorm2d-34           [-1, 32, 32, 32]              64\n",
      "AdaptiveAvgPool2d-35             [-1, 32, 1, 1]               0\n",
      "           Conv2d-36              [-1, 4, 1, 1]             132\n",
      "             ReLU-37              [-1, 4, 1, 1]               0\n",
      "           Conv2d-38             [-1, 32, 1, 1]             160\n",
      "          Sigmoid-39             [-1, 32, 1, 1]               0\n",
      "    SqueezeExcite-40           [-1, 32, 32, 32]               0\n",
      "             ReLU-41           [-1, 32, 32, 32]               0\n",
      "    ResidualBlock-42           [-1, 32, 32, 32]               0\n",
      "           Conv2d-43           [-1, 64, 16, 16]          18,432\n",
      "      BatchNorm2d-44           [-1, 64, 16, 16]             128\n",
      "             ReLU-45           [-1, 64, 16, 16]               0\n",
      "           Conv2d-46           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-47           [-1, 64, 16, 16]             128\n",
      "AdaptiveAvgPool2d-48             [-1, 64, 1, 1]               0\n",
      "           Conv2d-49              [-1, 8, 1, 1]             520\n",
      "             ReLU-50              [-1, 8, 1, 1]               0\n",
      "           Conv2d-51             [-1, 64, 1, 1]             576\n",
      "          Sigmoid-52             [-1, 64, 1, 1]               0\n",
      "    SqueezeExcite-53           [-1, 64, 16, 16]               0\n",
      "           Conv2d-54           [-1, 64, 16, 16]           2,048\n",
      "      BatchNorm2d-55           [-1, 64, 16, 16]             128\n",
      "             ReLU-56           [-1, 64, 16, 16]               0\n",
      "    ResidualBlock-57           [-1, 64, 16, 16]               0\n",
      "           Conv2d-58           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-59           [-1, 64, 16, 16]             128\n",
      "             ReLU-60           [-1, 64, 16, 16]               0\n",
      "           Conv2d-61           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-62           [-1, 64, 16, 16]             128\n",
      "AdaptiveAvgPool2d-63             [-1, 64, 1, 1]               0\n",
      "           Conv2d-64              [-1, 8, 1, 1]             520\n",
      "             ReLU-65              [-1, 8, 1, 1]               0\n",
      "           Conv2d-66             [-1, 64, 1, 1]             576\n",
      "          Sigmoid-67             [-1, 64, 1, 1]               0\n",
      "    SqueezeExcite-68           [-1, 64, 16, 16]               0\n",
      "             ReLU-69           [-1, 64, 16, 16]               0\n",
      "    ResidualBlock-70           [-1, 64, 16, 16]               0\n",
      "           Conv2d-71           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-72           [-1, 64, 16, 16]             128\n",
      "             ReLU-73           [-1, 64, 16, 16]               0\n",
      "           Conv2d-74           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-75           [-1, 64, 16, 16]             128\n",
      "AdaptiveAvgPool2d-76             [-1, 64, 1, 1]               0\n",
      "           Conv2d-77              [-1, 8, 1, 1]             520\n",
      "             ReLU-78              [-1, 8, 1, 1]               0\n",
      "           Conv2d-79             [-1, 64, 1, 1]             576\n",
      "          Sigmoid-80             [-1, 64, 1, 1]               0\n",
      "    SqueezeExcite-81           [-1, 64, 16, 16]               0\n",
      "             ReLU-82           [-1, 64, 16, 16]               0\n",
      "    ResidualBlock-83           [-1, 64, 16, 16]               0\n",
      "           Conv2d-84            [-1, 128, 8, 8]          73,728\n",
      "      BatchNorm2d-85            [-1, 128, 8, 8]             256\n",
      "             ReLU-86            [-1, 128, 8, 8]               0\n",
      "           Conv2d-87            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-88            [-1, 128, 8, 8]             256\n",
      "AdaptiveAvgPool2d-89            [-1, 128, 1, 1]               0\n",
      "           Conv2d-90             [-1, 16, 1, 1]           2,064\n",
      "             ReLU-91             [-1, 16, 1, 1]               0\n",
      "           Conv2d-92            [-1, 128, 1, 1]           2,176\n",
      "          Sigmoid-93            [-1, 128, 1, 1]               0\n",
      "    SqueezeExcite-94            [-1, 128, 8, 8]               0\n",
      "           Conv2d-95            [-1, 128, 8, 8]           8,192\n",
      "      BatchNorm2d-96            [-1, 128, 8, 8]             256\n",
      "             ReLU-97            [-1, 128, 8, 8]               0\n",
      "    ResidualBlock-98            [-1, 128, 8, 8]               0\n",
      "           Conv2d-99            [-1, 128, 8, 8]         147,456\n",
      "     BatchNorm2d-100            [-1, 128, 8, 8]             256\n",
      "            ReLU-101            [-1, 128, 8, 8]               0\n",
      "          Conv2d-102            [-1, 128, 8, 8]         147,456\n",
      "     BatchNorm2d-103            [-1, 128, 8, 8]             256\n",
      "AdaptiveAvgPool2d-104            [-1, 128, 1, 1]               0\n",
      "          Conv2d-105             [-1, 16, 1, 1]           2,064\n",
      "            ReLU-106             [-1, 16, 1, 1]               0\n",
      "          Conv2d-107            [-1, 128, 1, 1]           2,176\n",
      "         Sigmoid-108            [-1, 128, 1, 1]               0\n",
      "   SqueezeExcite-109            [-1, 128, 8, 8]               0\n",
      "            ReLU-110            [-1, 128, 8, 8]               0\n",
      "   ResidualBlock-111            [-1, 128, 8, 8]               0\n",
      "          Conv2d-112            [-1, 128, 8, 8]         147,456\n",
      "     BatchNorm2d-113            [-1, 128, 8, 8]             256\n",
      "            ReLU-114            [-1, 128, 8, 8]               0\n",
      "          Conv2d-115            [-1, 128, 8, 8]         147,456\n",
      "     BatchNorm2d-116            [-1, 128, 8, 8]             256\n",
      "AdaptiveAvgPool2d-117            [-1, 128, 1, 1]               0\n",
      "          Conv2d-118             [-1, 16, 1, 1]           2,064\n",
      "            ReLU-119             [-1, 16, 1, 1]               0\n",
      "          Conv2d-120            [-1, 128, 1, 1]           2,176\n",
      "         Sigmoid-121            [-1, 128, 1, 1]               0\n",
      "   SqueezeExcite-122            [-1, 128, 8, 8]               0\n",
      "            ReLU-123            [-1, 128, 8, 8]               0\n",
      "   ResidualBlock-124            [-1, 128, 8, 8]               0\n",
      "          Conv2d-125            [-1, 128, 8, 8]         147,456\n",
      "     BatchNorm2d-126            [-1, 128, 8, 8]             256\n",
      "            ReLU-127            [-1, 128, 8, 8]               0\n",
      "          Conv2d-128            [-1, 128, 8, 8]         147,456\n",
      "     BatchNorm2d-129            [-1, 128, 8, 8]             256\n",
      "AdaptiveAvgPool2d-130            [-1, 128, 1, 1]               0\n",
      "          Conv2d-131             [-1, 16, 1, 1]           2,064\n",
      "            ReLU-132             [-1, 16, 1, 1]               0\n",
      "          Conv2d-133            [-1, 128, 1, 1]           2,176\n",
      "         Sigmoid-134            [-1, 128, 1, 1]               0\n",
      "   SqueezeExcite-135            [-1, 128, 8, 8]               0\n",
      "            ReLU-136            [-1, 128, 8, 8]               0\n",
      "   ResidualBlock-137            [-1, 128, 8, 8]               0\n",
      "          Conv2d-138            [-1, 256, 4, 4]         294,912\n",
      "     BatchNorm2d-139            [-1, 256, 4, 4]             512\n",
      "            ReLU-140            [-1, 256, 4, 4]               0\n",
      "          Conv2d-141            [-1, 256, 4, 4]         589,824\n",
      "     BatchNorm2d-142            [-1, 256, 4, 4]             512\n",
      "AdaptiveAvgPool2d-143            [-1, 256, 1, 1]               0\n",
      "          Conv2d-144             [-1, 32, 1, 1]           8,224\n",
      "            ReLU-145             [-1, 32, 1, 1]               0\n",
      "          Conv2d-146            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-147            [-1, 256, 1, 1]               0\n",
      "   SqueezeExcite-148            [-1, 256, 4, 4]               0\n",
      "          Conv2d-149            [-1, 256, 4, 4]          32,768\n",
      "     BatchNorm2d-150            [-1, 256, 4, 4]             512\n",
      "            ReLU-151            [-1, 256, 4, 4]               0\n",
      "   ResidualBlock-152            [-1, 256, 4, 4]               0\n",
      "          Conv2d-153            [-1, 256, 4, 4]         589,824\n",
      "     BatchNorm2d-154            [-1, 256, 4, 4]             512\n",
      "            ReLU-155            [-1, 256, 4, 4]               0\n",
      "          Conv2d-156            [-1, 256, 4, 4]         589,824\n",
      "     BatchNorm2d-157            [-1, 256, 4, 4]             512\n",
      "AdaptiveAvgPool2d-158            [-1, 256, 1, 1]               0\n",
      "          Conv2d-159             [-1, 32, 1, 1]           8,224\n",
      "            ReLU-160             [-1, 32, 1, 1]               0\n",
      "          Conv2d-161            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-162            [-1, 256, 1, 1]               0\n",
      "   SqueezeExcite-163            [-1, 256, 4, 4]               0\n",
      "            ReLU-164            [-1, 256, 4, 4]               0\n",
      "   ResidualBlock-165            [-1, 256, 4, 4]               0\n",
      "          Conv2d-166            [-1, 256, 4, 4]         589,824\n",
      "     BatchNorm2d-167            [-1, 256, 4, 4]             512\n",
      "            ReLU-168            [-1, 256, 4, 4]               0\n",
      "          Conv2d-169            [-1, 256, 4, 4]         589,824\n",
      "     BatchNorm2d-170            [-1, 256, 4, 4]             512\n",
      "AdaptiveAvgPool2d-171            [-1, 256, 1, 1]               0\n",
      "          Conv2d-172             [-1, 32, 1, 1]           8,224\n",
      "            ReLU-173             [-1, 32, 1, 1]               0\n",
      "          Conv2d-174            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-175            [-1, 256, 1, 1]               0\n",
      "   SqueezeExcite-176            [-1, 256, 4, 4]               0\n",
      "            ReLU-177            [-1, 256, 4, 4]               0\n",
      "   ResidualBlock-178            [-1, 256, 4, 4]               0\n",
      "AdaptiveAvgPool2d-179            [-1, 256, 1, 1]               0\n",
      "         Dropout-180                  [-1, 256]               0\n",
      "          Linear-181                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,732,814\n",
      "Trainable params: 4,732,814\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 12.98\n",
      "Params size (MB): 18.05\n",
      "Estimated Total Size (MB): 31.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cl1Fd2ZTwHFY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define checkpoint saving interval\n",
    "save_interval = 5  # Save every 5 epochs\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)  # Create directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QTDC9gRVwMhc"
   },
   "outputs": [],
   "source": [
    "has_checkpoint = False\n",
    "if has_checkpoint == True:\n",
    "  checkpoint = torch.load(\"checkpoints/model_epoch_110.pth\")  # Adjust to the last saved epoch\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  start_epoch = checkpoint['epoch']\n",
    "  print(f\"Resuming from epoch {start_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWxZ1Pdb9frO",
    "outputId": "07e224e7-f54d-4112-b176-b42e0fc3d5d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    \"\"\"\n",
    "    Generate random bounding box coordinates for CutMix.\n",
    "\n",
    "    Args:\n",
    "        size (tuple): Size of the input tensor (batch, channels, width, height).\n",
    "        lam (float): Mixing ratio (lambda).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of bounding box coordinates (bbx1, bby1, bbx2, bby2).\n",
    "    \"\"\"\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)  # Compute cut ratio\n",
    "    cut_w = np.int64(W * cut_rat)\n",
    "    cut_h = np.int64(H * cut_rat)\n",
    "    \n",
    "    # Random center point for the bounding box\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    # Calculate bounding box coordinates and clip them to the image boundaries\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# ============================================\n",
    "# CutMix Augmentation Function\n",
    "# ============================================\n",
    "def cutmix_data(x, y, alpha=CUTMIX_ALPHA):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    y_a, y_b = y, y[index]\n",
    "\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "\n",
    "    # Create a boolean mask for the region to replace.\n",
    "    mask = torch.ones(x.size(), dtype=torch.bool, device=x.device)\n",
    "    mask[:, :, bbx1:bbx2, bby1:bby2] = False\n",
    "\n",
    "    # Use torch.where to combine images\n",
    "    x = torch.where(mask, x, x[index])\n",
    "    \n",
    "    # Adjust lambda to reflect the actual area replaced\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Mixup Augmentation Function\n",
    "# ============================================\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    \"\"\"\n",
    "    Applies Mixup augmentation by linear interpolation between pairs of images.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input images.\n",
    "        y (Tensor): Labels.\n",
    "        alpha (float): Beta distribution parameter.\n",
    "\n",
    "    Returns:\n",
    "        mixed_x (Tensor): Mixed images.\n",
    "        y_a (Tensor): Original labels.\n",
    "        y_b (Tensor): Shuffled labels.\n",
    "        lam (float): Mixup coefficient.\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ============================================\n",
    "# Mixup Criterion Function\n",
    "# ============================================\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"\n",
    "    Computes the mixup loss as a linear combination of the losses on the mixed pairs.\n",
    "\n",
    "    Args:\n",
    "        criterion: Loss function.\n",
    "        pred (Tensor): Model predictions.\n",
    "        y_a (Tensor): Original labels.\n",
    "        y_b (Tensor): Shuffled labels.\n",
    "        lam (float): Mixup coefficient.\n",
    "\n",
    "    Returns:\n",
    "        A scalar loss value.\n",
    "    \"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ============================================\n",
    "# Advanced Mixup Function\n",
    "# ============================================\n",
    "def advanced_mixup_data(x, y):\n",
    "    \"\"\"\n",
    "    Applies either Mixup or CutMix augmentation based on a predefined probability.\n",
    "\n",
    "    Returns:\n",
    "        Mixed inputs, paired labels, and mixup coefficient.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < ADVANCED_MIXUP_PROB:\n",
    "        return cutmix_data(x, y, alpha=CUTMIX_ALPHA)\n",
    "    else:\n",
    "        return mixup_data(x, y, alpha=MIXUP_ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRFJ29GyfXy5",
    "outputId": "291a82db-9fe0-44a0-e49e-e9bf2233ae36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 2.3524, Train Accuracy: 13.72%\n",
      "Validation Accuracy: 20.91 %\n",
      "Epoch 1 completed. Current LR: 0.004\n",
      "Epoch [2/300], Loss: 2.2005, Train Accuracy: 19.03%\n",
      "Validation Accuracy: 26.16 %\n",
      "Epoch 2 completed. Current LR: 0.006\n",
      "Epoch [3/300], Loss: 2.1270, Train Accuracy: 22.84%\n",
      "Validation Accuracy: 30.40 %\n",
      "Epoch 3 completed. Current LR: 0.008\n",
      "Epoch [4/300], Loss: 2.0705, Train Accuracy: 26.77%\n",
      "Validation Accuracy: 35.87 %\n",
      "Epoch 4 completed. Current LR: 0.01\n",
      "Epoch [5/300], Loss: 2.0327, Train Accuracy: 29.23%\n",
      "Validation Accuracy: 38.88 %\n",
      "Checkpoint saved at checkpoints/model_epoch_5.pth\n",
      "Epoch 5 completed. Current LR: 0.012\n",
      "Epoch [6/300], Loss: 1.9630, Train Accuracy: 33.35%\n",
      "Validation Accuracy: 38.45 %\n",
      "Epoch 6 completed. Current LR: 0.013999999999999999\n",
      "Epoch [7/300], Loss: 1.9124, Train Accuracy: 36.35%\n",
      "Validation Accuracy: 48.43 %\n",
      "Epoch 7 completed. Current LR: 0.016\n",
      "Epoch [8/300], Loss: 1.8742, Train Accuracy: 38.58%\n",
      "Validation Accuracy: 50.93 %\n",
      "Epoch 8 completed. Current LR: 0.018000000000000002\n",
      "Epoch [9/300], Loss: 1.8253, Train Accuracy: 41.51%\n",
      "Validation Accuracy: 52.15 %\n",
      "Epoch 9 completed. Current LR: 0.02\n",
      "Epoch [10/300], Loss: 1.8098, Train Accuracy: 42.47%\n",
      "Validation Accuracy: 55.39 %\n",
      "Checkpoint saved at checkpoints/model_epoch_10.pth\n",
      "Epoch 10 completed. Current LR: 0.02\n",
      "Epoch [11/300], Loss: 1.7654, Train Accuracy: 44.85%\n",
      "Validation Accuracy: 56.60 %\n",
      "Epoch 11 completed. Current LR: 0.019999413227831133\n",
      "Epoch [12/300], Loss: 1.7437, Train Accuracy: 46.29%\n",
      "Validation Accuracy: 57.51 %\n",
      "Epoch 12 completed. Current LR: 0.01999765298018484\n",
      "Epoch [13/300], Loss: 1.7081, Train Accuracy: 47.70%\n",
      "Validation Accuracy: 60.92 %\n",
      "Epoch 13 completed. Current LR: 0.019994719463633993\n",
      "Epoch [14/300], Loss: 1.6900, Train Accuracy: 49.03%\n",
      "Validation Accuracy: 64.99 %\n",
      "Epoch 14 completed. Current LR: 0.019990613022439766\n",
      "Epoch [15/300], Loss: 1.6789, Train Accuracy: 49.69%\n",
      "Validation Accuracy: 66.23 %\n",
      "Checkpoint saved at checkpoints/model_epoch_15.pth\n",
      "Epoch 15 completed. Current LR: 0.019985334138511238\n",
      "Epoch [16/300], Loss: 1.6428, Train Accuracy: 51.49%\n",
      "Validation Accuracy: 63.09 %\n",
      "Epoch 16 completed. Current LR: 0.019978883431348845\n",
      "Epoch [17/300], Loss: 1.6509, Train Accuracy: 51.10%\n",
      "Validation Accuracy: 68.95 %\n",
      "Epoch 17 completed. Current LR: 0.019971261657971667\n",
      "Epoch [18/300], Loss: 1.6264, Train Accuracy: 52.41%\n",
      "Validation Accuracy: 66.09 %\n",
      "Epoch 18 completed. Current LR: 0.019962469712828612\n",
      "Epoch [19/300], Loss: 1.6071, Train Accuracy: 53.45%\n",
      "Validation Accuracy: 69.96 %\n",
      "Epoch 19 completed. Current LR: 0.01995250862769342\n",
      "Epoch [20/300], Loss: 1.6170, Train Accuracy: 52.92%\n",
      "Validation Accuracy: 60.20 %\n",
      "Checkpoint saved at checkpoints/model_epoch_20.pth\n",
      "Epoch 20 completed. Current LR: 0.019941379571543596\n",
      "Epoch [21/300], Loss: 1.5940, Train Accuracy: 54.23%\n",
      "Validation Accuracy: 64.63 %\n",
      "Epoch 21 completed. Current LR: 0.019929083850423225\n",
      "Epoch [22/300], Loss: 1.6161, Train Accuracy: 53.04%\n",
      "Validation Accuracy: 69.13 %\n",
      "Epoch 22 completed. Current LR: 0.019915622907289696\n",
      "Epoch [23/300], Loss: 1.5744, Train Accuracy: 54.77%\n",
      "Validation Accuracy: 68.88 %\n",
      "Epoch 23 completed. Current LR: 0.019900998321844367\n",
      "Epoch [24/300], Loss: 1.5911, Train Accuracy: 54.19%\n",
      "Validation Accuracy: 65.07 %\n",
      "Epoch 24 completed. Current LR: 0.019885211810347185\n",
      "Epoch [25/300], Loss: 1.5544, Train Accuracy: 55.96%\n",
      "Validation Accuracy: 70.31 %\n",
      "Checkpoint saved at checkpoints/model_epoch_25.pth\n",
      "Epoch 25 completed. Current LR: 0.019868265225415262\n",
      "Epoch [26/300], Loss: 1.5742, Train Accuracy: 55.05%\n",
      "Validation Accuracy: 70.21 %\n",
      "Epoch 26 completed. Current LR: 0.019850160555805487\n",
      "Epoch [27/300], Loss: 1.5754, Train Accuracy: 54.93%\n",
      "Validation Accuracy: 69.05 %\n",
      "Epoch 27 completed. Current LR: 0.0198308999261811\n",
      "Epoch [28/300], Loss: 1.5536, Train Accuracy: 56.08%\n",
      "Validation Accuracy: 70.63 %\n",
      "Epoch 28 completed. Current LR: 0.019810485596862392\n",
      "Epoch [29/300], Loss: 1.5473, Train Accuracy: 56.04%\n",
      "Validation Accuracy: 69.57 %\n",
      "Epoch 29 completed. Current LR: 0.01978891996356142\n",
      "Epoch [30/300], Loss: 1.5466, Train Accuracy: 56.38%\n",
      "Validation Accuracy: 67.80 %\n",
      "Checkpoint saved at checkpoints/model_epoch_30.pth\n",
      "Epoch 30 completed. Current LR: 0.019766205557100867\n",
      "Epoch [31/300], Loss: 1.5400, Train Accuracy: 56.55%\n",
      "Validation Accuracy: 69.71 %\n",
      "Epoch 31 completed. Current LR: 0.019742345043117046\n",
      "Epoch [32/300], Loss: 1.5564, Train Accuracy: 55.81%\n",
      "Validation Accuracy: 73.20 %\n",
      "Epoch 32 completed. Current LR: 0.019717341221747058\n",
      "Epoch [33/300], Loss: 1.5354, Train Accuracy: 56.62%\n",
      "Validation Accuracy: 73.08 %\n",
      "Epoch 33 completed. Current LR: 0.019691197027300204\n",
      "Epoch [34/300], Loss: 1.5240, Train Accuracy: 57.22%\n",
      "Validation Accuracy: 75.37 %\n",
      "Epoch 34 completed. Current LR: 0.019663915527913627\n",
      "Epoch [35/300], Loss: 1.5047, Train Accuracy: 57.99%\n",
      "Validation Accuracy: 73.87 %\n",
      "Checkpoint saved at checkpoints/model_epoch_35.pth\n",
      "Epoch 35 completed. Current LR: 0.01963549992519223\n",
      "Epoch [36/300], Loss: 1.5214, Train Accuracy: 57.45%\n",
      "Validation Accuracy: 73.91 %\n",
      "Epoch 36 completed. Current LR: 0.019605953553832985\n",
      "Epoch [37/300], Loss: 1.5044, Train Accuracy: 58.25%\n",
      "Validation Accuracy: 71.81 %\n",
      "Epoch 37 completed. Current LR: 0.019575279881233577\n",
      "Epoch [38/300], Loss: 1.5373, Train Accuracy: 56.81%\n",
      "Validation Accuracy: 73.91 %\n",
      "Epoch 38 completed. Current LR: 0.019543482507085482\n",
      "Epoch [39/300], Loss: 1.5146, Train Accuracy: 57.80%\n",
      "Validation Accuracy: 74.60 %\n",
      "Epoch 39 completed. Current LR: 0.019510565162951538\n",
      "Epoch [40/300], Loss: 1.4975, Train Accuracy: 58.55%\n",
      "Validation Accuracy: 73.48 %\n",
      "Checkpoint saved at checkpoints/model_epoch_40.pth\n",
      "Epoch 40 completed. Current LR: 0.019476531711828025\n",
      "Epoch [41/300], Loss: 1.5085, Train Accuracy: 58.16%\n",
      "Validation Accuracy: 75.16 %\n",
      "Epoch 41 completed. Current LR: 0.019441386147691333\n",
      "Epoch [42/300], Loss: 1.5171, Train Accuracy: 57.92%\n",
      "Validation Accuracy: 71.19 %\n",
      "Epoch 42 completed. Current LR: 0.01940513259502924\n",
      "Epoch [43/300], Loss: 1.4967, Train Accuracy: 58.63%\n",
      "Validation Accuracy: 73.49 %\n",
      "Epoch 43 completed. Current LR: 0.01936777530835689\n",
      "Epoch [44/300], Loss: 1.5181, Train Accuracy: 57.67%\n",
      "Validation Accuracy: 76.40 %\n",
      "Epoch 44 completed. Current LR: 0.01932931867171751\n",
      "Epoch [45/300], Loss: 1.5020, Train Accuracy: 58.45%\n",
      "Validation Accuracy: 74.83 %\n",
      "Checkpoint saved at checkpoints/model_epoch_45.pth\n",
      "Epoch 45 completed. Current LR: 0.019289767198167915\n",
      "Epoch [46/300], Loss: 1.5049, Train Accuracy: 58.44%\n",
      "Validation Accuracy: 74.17 %\n",
      "Epoch 46 completed. Current LR: 0.01924912552924889\n",
      "Epoch [47/300], Loss: 1.5090, Train Accuracy: 58.04%\n",
      "Validation Accuracy: 73.77 %\n",
      "Epoch 47 completed. Current LR: 0.019207398434440477\n",
      "Epoch [48/300], Loss: 1.4812, Train Accuracy: 59.20%\n",
      "Validation Accuracy: 72.60 %\n",
      "Epoch 48 completed. Current LR: 0.01916459081060226\n",
      "Epoch [49/300], Loss: 1.4788, Train Accuracy: 59.28%\n",
      "Validation Accuracy: 74.32 %\n",
      "Epoch 49 completed. Current LR: 0.0191207076813987\n",
      "Epoch [50/300], Loss: 1.5059, Train Accuracy: 58.17%\n",
      "Validation Accuracy: 75.95 %\n",
      "Checkpoint saved at checkpoints/model_epoch_50.pth\n",
      "Epoch 50 completed. Current LR: 0.019075754196709573\n",
      "Epoch [51/300], Loss: 1.5016, Train Accuracy: 58.42%\n",
      "Validation Accuracy: 75.68 %\n",
      "Epoch 51 completed. Current LR: 0.01902973563202562\n",
      "Epoch [52/300], Loss: 1.4952, Train Accuracy: 58.83%\n",
      "Validation Accuracy: 76.00 %\n",
      "Epoch 52 completed. Current LR: 0.018982657387829446\n",
      "Epoch [53/300], Loss: 1.4924, Train Accuracy: 58.93%\n",
      "Validation Accuracy: 72.25 %\n",
      "Epoch 53 completed. Current LR: 0.018934524988961737\n",
      "Epoch [54/300], Loss: 1.4749, Train Accuracy: 59.60%\n",
      "Validation Accuracy: 73.92 %\n",
      "Epoch 54 completed. Current LR: 0.018885344083972914\n",
      "Epoch [55/300], Loss: 1.5028, Train Accuracy: 58.46%\n",
      "Validation Accuracy: 74.97 %\n",
      "Checkpoint saved at checkpoints/model_epoch_55.pth\n",
      "Epoch 55 completed. Current LR: 0.01883512044446023\n",
      "Epoch [56/300], Loss: 1.4689, Train Accuracy: 59.92%\n",
      "Validation Accuracy: 75.81 %\n",
      "Epoch 56 completed. Current LR: 0.018783859964390463\n",
      "Epoch [57/300], Loss: 1.4746, Train Accuracy: 59.61%\n",
      "Validation Accuracy: 74.59 %\n",
      "Epoch 57 completed. Current LR: 0.01873156865940823\n",
      "Epoch [58/300], Loss: 1.4824, Train Accuracy: 59.46%\n",
      "Validation Accuracy: 74.93 %\n",
      "Epoch 58 completed. Current LR: 0.018678252666130015\n",
      "Epoch [59/300], Loss: 1.4650, Train Accuracy: 60.17%\n",
      "Validation Accuracy: 75.44 %\n",
      "Epoch 59 completed. Current LR: 0.01862391824142402\n",
      "Epoch [60/300], Loss: 1.4677, Train Accuracy: 60.14%\n",
      "Validation Accuracy: 78.17 %\n",
      "Checkpoint saved at checkpoints/model_epoch_60.pth\n",
      "Epoch 60 completed. Current LR: 0.01856857176167589\n",
      "Epoch [61/300], Loss: 1.4705, Train Accuracy: 59.64%\n",
      "Validation Accuracy: 71.37 %\n",
      "Epoch 61 completed. Current LR: 0.018512219722040424\n",
      "Epoch [62/300], Loss: 1.4748, Train Accuracy: 59.80%\n",
      "Validation Accuracy: 72.21 %\n",
      "Epoch 62 completed. Current LR: 0.01845486873567932\n",
      "Epoch [63/300], Loss: 1.4891, Train Accuracy: 59.09%\n",
      "Validation Accuracy: 76.16 %\n",
      "Epoch 63 completed. Current LR: 0.018396525532985108\n",
      "Epoch [64/300], Loss: 1.4927, Train Accuracy: 58.91%\n",
      "Validation Accuracy: 76.43 %\n",
      "Epoch 64 completed. Current LR: 0.018337196960791304\n",
      "Epoch [65/300], Loss: 1.4730, Train Accuracy: 59.82%\n",
      "Validation Accuracy: 74.33 %\n",
      "Checkpoint saved at checkpoints/model_epoch_65.pth\n",
      "Epoch 65 completed. Current LR: 0.018276889981568907\n",
      "Epoch [66/300], Loss: 1.4867, Train Accuracy: 59.27%\n",
      "Validation Accuracy: 75.56 %\n",
      "Epoch 66 completed. Current LR: 0.018215611672609315\n",
      "Epoch [67/300], Loss: 1.4915, Train Accuracy: 58.91%\n",
      "Validation Accuracy: 78.25 %\n",
      "Epoch 67 completed. Current LR: 0.01815336922519378\n",
      "Epoch [68/300], Loss: 1.4504, Train Accuracy: 60.80%\n",
      "Validation Accuracy: 76.91 %\n",
      "Epoch 68 completed. Current LR: 0.018090169943749474\n",
      "Epoch [69/300], Loss: 1.4703, Train Accuracy: 59.80%\n",
      "Validation Accuracy: 76.47 %\n",
      "Epoch 69 completed. Current LR: 0.018026021244992287\n",
      "Epoch [70/300], Loss: 1.4520, Train Accuracy: 60.63%\n",
      "Validation Accuracy: 76.85 %\n",
      "Checkpoint saved at checkpoints/model_epoch_70.pth\n",
      "Epoch 70 completed. Current LR: 0.017960930657056437\n",
      "Epoch [71/300], Loss: 1.4623, Train Accuracy: 60.43%\n",
      "Validation Accuracy: 76.36 %\n",
      "Epoch 71 completed. Current LR: 0.01789490581861102\n",
      "Epoch [72/300], Loss: 1.4560, Train Accuracy: 60.57%\n",
      "Validation Accuracy: 77.05 %\n",
      "Epoch 72 completed. Current LR: 0.017827954477963558\n",
      "Epoch [73/300], Loss: 1.4682, Train Accuracy: 60.19%\n",
      "Validation Accuracy: 77.20 %\n",
      "Epoch 73 completed. Current LR: 0.01776008449215073\n",
      "Epoch [74/300], Loss: 1.4518, Train Accuracy: 60.71%\n",
      "Validation Accuracy: 76.96 %\n",
      "Epoch 74 completed. Current LR: 0.01769130382601629\n",
      "Epoch [75/300], Loss: 1.4573, Train Accuracy: 60.50%\n",
      "Validation Accuracy: 75.49 %\n",
      "Checkpoint saved at checkpoints/model_epoch_75.pth\n",
      "Epoch 75 completed. Current LR: 0.017621620551276366\n",
      "Epoch [76/300], Loss: 1.4791, Train Accuracy: 59.48%\n",
      "Validation Accuracy: 78.29 %\n",
      "Epoch 76 completed. Current LR: 0.01755104284557221\n",
      "Epoch [77/300], Loss: 1.4623, Train Accuracy: 60.05%\n",
      "Validation Accuracy: 79.17 %\n",
      "Epoch 77 completed. Current LR: 0.017479578991510505\n",
      "Epoch [78/300], Loss: 1.4481, Train Accuracy: 60.91%\n",
      "Validation Accuracy: 76.72 %\n",
      "Epoch 78 completed. Current LR: 0.01740723737569139\n",
      "Epoch [79/300], Loss: 1.4703, Train Accuracy: 59.61%\n",
      "Validation Accuracy: 76.79 %\n",
      "Epoch 79 completed. Current LR: 0.017334026487724224\n",
      "Epoch [80/300], Loss: 1.4696, Train Accuracy: 60.21%\n",
      "Validation Accuracy: 76.16 %\n",
      "Checkpoint saved at checkpoints/model_epoch_80.pth\n",
      "Epoch 80 completed. Current LR: 0.01725995491923131\n",
      "Epoch [81/300], Loss: 1.4571, Train Accuracy: 60.61%\n",
      "Validation Accuracy: 77.81 %\n",
      "Epoch 81 completed. Current LR: 0.017185031362839626\n",
      "Epoch [82/300], Loss: 1.4757, Train Accuracy: 59.85%\n",
      "Validation Accuracy: 77.00 %\n",
      "Epoch 82 completed. Current LR: 0.017109264611160708\n",
      "Epoch [83/300], Loss: 1.4453, Train Accuracy: 61.34%\n",
      "Validation Accuracy: 73.55 %\n",
      "Epoch 83 completed. Current LR: 0.0170326635557588\n",
      "Epoch [84/300], Loss: 1.4447, Train Accuracy: 61.10%\n",
      "Validation Accuracy: 77.56 %\n",
      "Epoch 84 completed. Current LR: 0.016955237186107388\n",
      "Epoch [85/300], Loss: 1.4522, Train Accuracy: 60.54%\n",
      "Validation Accuracy: 77.37 %\n",
      "Checkpoint saved at checkpoints/model_epoch_85.pth\n",
      "Epoch 85 completed. Current LR: 0.016876994588534235\n",
      "Epoch [86/300], Loss: 1.4569, Train Accuracy: 60.46%\n",
      "Validation Accuracy: 75.05 %\n",
      "Epoch 86 completed. Current LR: 0.016797944945155078\n",
      "Epoch [87/300], Loss: 1.4251, Train Accuracy: 62.11%\n",
      "Validation Accuracy: 76.23 %\n",
      "Epoch 87 completed. Current LR: 0.01671809753279606\n",
      "Epoch [88/300], Loss: 1.4531, Train Accuracy: 60.54%\n",
      "Validation Accuracy: 77.53 %\n",
      "Epoch 88 completed. Current LR: 0.016637461721905045\n",
      "Epoch [89/300], Loss: 1.4301, Train Accuracy: 61.59%\n",
      "Validation Accuracy: 77.28 %\n",
      "Epoch 89 completed. Current LR: 0.01655604697545196\n",
      "Epoch [90/300], Loss: 1.4657, Train Accuracy: 60.29%\n",
      "Validation Accuracy: 78.29 %\n",
      "Checkpoint saved at checkpoints/model_epoch_90.pth\n",
      "Epoch 90 completed. Current LR: 0.016473862847818276\n",
      "Epoch [91/300], Loss: 1.4286, Train Accuracy: 61.90%\n",
      "Validation Accuracy: 79.11 %\n",
      "Epoch 91 completed. Current LR: 0.01639091898367576\n",
      "Epoch [92/300], Loss: 1.4489, Train Accuracy: 60.76%\n",
      "Validation Accuracy: 78.85 %\n",
      "Epoch 92 completed. Current LR: 0.01630722511685462\n",
      "Epoch [93/300], Loss: 1.4315, Train Accuracy: 61.79%\n",
      "Validation Accuracy: 78.11 %\n",
      "Epoch 93 completed. Current LR: 0.016222791069201208\n",
      "Epoch [94/300], Loss: 1.4300, Train Accuracy: 61.84%\n",
      "Validation Accuracy: 79.87 %\n",
      "Epoch 94 completed. Current LR: 0.016137626749425375\n",
      "Epoch [95/300], Loss: 1.4197, Train Accuracy: 62.39%\n",
      "Validation Accuracy: 77.25 %\n",
      "Checkpoint saved at checkpoints/model_epoch_95.pth\n",
      "Epoch 95 completed. Current LR: 0.01605174215193765\n",
      "Epoch [96/300], Loss: 1.4431, Train Accuracy: 61.19%\n",
      "Validation Accuracy: 79.13 %\n",
      "Epoch 96 completed. Current LR: 0.015965147355676345\n",
      "Epoch [97/300], Loss: 1.4293, Train Accuracy: 61.78%\n",
      "Validation Accuracy: 79.23 %\n",
      "Epoch 97 completed. Current LR: 0.015877852522924733\n",
      "Epoch [98/300], Loss: 1.4304, Train Accuracy: 61.70%\n",
      "Validation Accuracy: 79.40 %\n",
      "Epoch 98 completed. Current LR: 0.01578986789811849\n",
      "Epoch [99/300], Loss: 1.4162, Train Accuracy: 62.34%\n",
      "Validation Accuracy: 78.33 %\n",
      "Epoch 99 completed. Current LR: 0.015701203806643433\n",
      "Epoch [100/300], Loss: 1.4563, Train Accuracy: 60.57%\n",
      "Validation Accuracy: 78.55 %\n",
      "Checkpoint saved at checkpoints/model_epoch_100.pth\n",
      "Epoch 100 completed. Current LR: 0.015611870653623825\n",
      "Epoch [101/300], Loss: 1.4296, Train Accuracy: 61.87%\n",
      "Validation Accuracy: 78.39 %\n",
      "Epoch 101 completed. Current LR: 0.015521878922701246\n",
      "Epoch [102/300], Loss: 1.4474, Train Accuracy: 60.80%\n",
      "Validation Accuracy: 80.08 %\n",
      "Epoch 102 completed. Current LR: 0.01543123917480433\n",
      "Epoch [103/300], Loss: 1.4275, Train Accuracy: 61.88%\n",
      "Validation Accuracy: 79.45 %\n",
      "Epoch 103 completed. Current LR: 0.015339962046909364\n",
      "Epoch [104/300], Loss: 1.4352, Train Accuracy: 61.37%\n",
      "Validation Accuracy: 77.96 %\n",
      "Epoch 104 completed. Current LR: 0.015248058250792007\n",
      "Epoch [105/300], Loss: 1.4105, Train Accuracy: 62.84%\n",
      "Validation Accuracy: 78.17 %\n",
      "Checkpoint saved at checkpoints/model_epoch_105.pth\n",
      "Epoch 105 completed. Current LR: 0.015155538571770218\n",
      "Epoch [106/300], Loss: 1.4054, Train Accuracy: 62.86%\n",
      "Validation Accuracy: 79.25 %\n",
      "Epoch 106 completed. Current LR: 0.01506241386743854\n",
      "Epoch [107/300], Loss: 1.4216, Train Accuracy: 62.10%\n",
      "Validation Accuracy: 78.61 %\n",
      "Epoch 107 completed. Current LR: 0.014968695066393922\n",
      "Epoch [108/300], Loss: 1.4359, Train Accuracy: 61.62%\n",
      "Validation Accuracy: 78.89 %\n",
      "Epoch 108 completed. Current LR: 0.014874393166953192\n",
      "Epoch [109/300], Loss: 1.4056, Train Accuracy: 62.96%\n",
      "Validation Accuracy: 79.65 %\n",
      "Epoch 109 completed. Current LR: 0.014779519235862364\n",
      "Epoch [110/300], Loss: 1.4300, Train Accuracy: 61.83%\n",
      "Validation Accuracy: 78.11 %\n",
      "Checkpoint saved at checkpoints/model_epoch_110.pth\n",
      "Epoch 110 completed. Current LR: 0.014684084406997902\n",
      "Epoch [111/300], Loss: 1.4036, Train Accuracy: 63.05%\n",
      "Validation Accuracy: 78.79 %\n",
      "Epoch 111 completed. Current LR: 0.014588099880060108\n",
      "Epoch [112/300], Loss: 1.4115, Train Accuracy: 62.67%\n",
      "Validation Accuracy: 76.04 %\n",
      "Epoch 112 completed. Current LR: 0.014491576919258791\n",
      "Epoch [113/300], Loss: 1.4049, Train Accuracy: 62.69%\n",
      "Validation Accuracy: 80.59 %\n",
      "Epoch 113 completed. Current LR: 0.014394526851991362\n",
      "Epoch [114/300], Loss: 1.4256, Train Accuracy: 61.78%\n",
      "Validation Accuracy: 78.04 %\n",
      "Epoch 114 completed. Current LR: 0.014296961067513516\n",
      "Epoch [115/300], Loss: 1.4176, Train Accuracy: 62.30%\n",
      "Validation Accuracy: 76.93 %\n",
      "Checkpoint saved at checkpoints/model_epoch_115.pth\n",
      "Epoch 115 completed. Current LR: 0.014198891015602646\n",
      "Epoch [116/300], Loss: 1.4341, Train Accuracy: 61.62%\n",
      "Validation Accuracy: 78.84 %\n",
      "Epoch 116 completed. Current LR: 0.01410032820521416\n",
      "Epoch [117/300], Loss: 1.4052, Train Accuracy: 62.82%\n",
      "Validation Accuracy: 80.48 %\n",
      "Epoch 117 completed. Current LR: 0.014001284203130866\n",
      "Epoch [118/300], Loss: 1.4252, Train Accuracy: 62.07%\n",
      "Validation Accuracy: 80.11 %\n",
      "Epoch 118 completed. Current LR: 0.013901770632605545\n",
      "Epoch [119/300], Loss: 1.4172, Train Accuracy: 62.53%\n",
      "Validation Accuracy: 78.91 %\n",
      "Epoch 119 completed. Current LR: 0.01380179917199692\n",
      "Epoch [120/300], Loss: 1.4099, Train Accuracy: 62.69%\n",
      "Validation Accuracy: 78.57 %\n",
      "Checkpoint saved at checkpoints/model_epoch_120.pth\n",
      "Epoch 120 completed. Current LR: 0.013701381553399145\n",
      "Epoch [121/300], Loss: 1.4211, Train Accuracy: 62.06%\n",
      "Validation Accuracy: 80.01 %\n",
      "Epoch 121 completed. Current LR: 0.01360052956126499\n",
      "Epoch [122/300], Loss: 1.3995, Train Accuracy: 63.41%\n",
      "Validation Accuracy: 80.67 %\n",
      "Epoch 122 completed. Current LR: 0.013499255031022886\n",
      "Epoch [123/300], Loss: 1.4186, Train Accuracy: 62.24%\n",
      "Validation Accuracy: 79.69 %\n",
      "Epoch 123 completed. Current LR: 0.013397569847687986\n",
      "Epoch [124/300], Loss: 1.4204, Train Accuracy: 62.33%\n",
      "Validation Accuracy: 79.88 %\n",
      "Epoch 124 completed. Current LR: 0.013295485944467405\n",
      "Epoch [125/300], Loss: 1.4060, Train Accuracy: 62.65%\n",
      "Validation Accuracy: 79.23 %\n",
      "Checkpoint saved at checkpoints/model_epoch_125.pth\n",
      "Epoch 125 completed. Current LR: 0.013193015301359799\n",
      "Epoch [126/300], Loss: 1.3997, Train Accuracy: 63.04%\n",
      "Validation Accuracy: 80.20 %\n",
      "Epoch 126 completed. Current LR: 0.013090169943749475\n",
      "Epoch [127/300], Loss: 1.4075, Train Accuracy: 62.84%\n",
      "Validation Accuracy: 79.07 %\n",
      "Epoch 127 completed. Current LR: 0.012986961940995136\n",
      "Epoch [128/300], Loss: 1.3931, Train Accuracy: 63.27%\n",
      "Validation Accuracy: 79.56 %\n",
      "Epoch 128 completed. Current LR: 0.012883403405013507\n",
      "Epoch [129/300], Loss: 1.4018, Train Accuracy: 62.83%\n",
      "Validation Accuracy: 79.45 %\n",
      "Epoch 129 completed. Current LR: 0.012779506488857943\n",
      "Epoch [130/300], Loss: 1.4047, Train Accuracy: 62.76%\n",
      "Validation Accuracy: 79.45 %\n",
      "Checkpoint saved at checkpoints/model_epoch_130.pth\n",
      "Epoch 130 completed. Current LR: 0.01267528338529221\n",
      "Epoch [131/300], Loss: 1.4049, Train Accuracy: 63.03%\n",
      "Validation Accuracy: 81.71 %\n",
      "Epoch 131 completed. Current LR: 0.012570746325359608\n",
      "Epoch [132/300], Loss: 1.3905, Train Accuracy: 63.51%\n",
      "Validation Accuracy: 81.45 %\n",
      "Epoch 132 completed. Current LR: 0.012465907576947622\n",
      "Epoch [133/300], Loss: 1.4096, Train Accuracy: 62.52%\n",
      "Validation Accuracy: 79.71 %\n",
      "Epoch 133 completed. Current LR: 0.0123607794433482\n",
      "Epoch [134/300], Loss: 1.3840, Train Accuracy: 63.90%\n",
      "Validation Accuracy: 81.04 %\n",
      "Epoch 134 completed. Current LR: 0.012255374261813943\n",
      "Epoch [135/300], Loss: 1.4142, Train Accuracy: 62.52%\n",
      "Validation Accuracy: 81.23 %\n",
      "Checkpoint saved at checkpoints/model_epoch_135.pth\n",
      "Epoch 135 completed. Current LR: 0.012149704402110242\n",
      "Epoch [136/300], Loss: 1.3904, Train Accuracy: 63.42%\n",
      "Validation Accuracy: 81.56 %\n",
      "Epoch 136 completed. Current LR: 0.01204378226506365\n",
      "Epoch [137/300], Loss: 1.4089, Train Accuracy: 62.68%\n",
      "Validation Accuracy: 81.27 %\n",
      "Epoch 137 completed. Current LR: 0.011937620281106585\n",
      "Epoch [138/300], Loss: 1.3727, Train Accuracy: 64.25%\n",
      "Validation Accuracy: 80.35 %\n",
      "Epoch 138 completed. Current LR: 0.01183123090881856\n",
      "Epoch [139/300], Loss: 1.3879, Train Accuracy: 63.84%\n",
      "Validation Accuracy: 81.23 %\n",
      "Epoch 139 completed. Current LR: 0.011724626633464127\n",
      "Epoch [140/300], Loss: 1.3712, Train Accuracy: 64.51%\n",
      "Validation Accuracy: 80.28 %\n",
      "Checkpoint saved at checkpoints/model_epoch_140.pth\n",
      "Epoch 140 completed. Current LR: 0.011617819965527649\n",
      "Epoch [141/300], Loss: 1.3875, Train Accuracy: 63.57%\n",
      "Validation Accuracy: 81.55 %\n",
      "Epoch 141 completed. Current LR: 0.011510823439245168\n",
      "Epoch [142/300], Loss: 1.3939, Train Accuracy: 63.26%\n",
      "Validation Accuracy: 81.91 %\n",
      "Epoch 142 completed. Current LR: 0.011403649611133444\n",
      "Epoch [143/300], Loss: 1.3712, Train Accuracy: 64.33%\n",
      "Validation Accuracy: 80.35 %\n",
      "Epoch 143 completed. Current LR: 0.011296311058516386\n",
      "Epoch [144/300], Loss: 1.3635, Train Accuracy: 64.82%\n",
      "Validation Accuracy: 80.73 %\n",
      "Epoch 144 completed. Current LR: 0.011188820378049065\n",
      "Epoch [145/300], Loss: 1.3869, Train Accuracy: 63.60%\n",
      "Validation Accuracy: 82.13 %\n",
      "Checkpoint saved at checkpoints/model_epoch_145.pth\n",
      "Epoch 145 completed. Current LR: 0.011081190184239418\n",
      "Epoch [146/300], Loss: 1.4000, Train Accuracy: 63.08%\n",
      "Validation Accuracy: 81.37 %\n",
      "Epoch 146 completed. Current LR: 0.010973433107967896\n",
      "Epoch [147/300], Loss: 1.3759, Train Accuracy: 63.93%\n",
      "Validation Accuracy: 80.76 %\n",
      "Epoch 147 completed. Current LR: 0.010865561795005177\n",
      "Epoch [148/300], Loss: 1.3899, Train Accuracy: 63.44%\n",
      "Validation Accuracy: 80.61 %\n",
      "Epoch 148 completed. Current LR: 0.010757588904528105\n",
      "Epoch [149/300], Loss: 1.3766, Train Accuracy: 64.17%\n",
      "Validation Accuracy: 81.19 %\n",
      "Epoch 149 completed. Current LR: 0.010649527107634107\n",
      "Epoch [150/300], Loss: 1.4146, Train Accuracy: 62.65%\n",
      "Validation Accuracy: 78.84 %\n",
      "Checkpoint saved at checkpoints/model_epoch_150.pth\n",
      "Epoch 150 completed. Current LR: 0.010541389085854176\n",
      "Epoch [151/300], Loss: 1.3868, Train Accuracy: 63.74%\n",
      "Validation Accuracy: 80.41 %\n",
      "Epoch 151 completed. Current LR: 0.010433187529664623\n",
      "Epoch [152/300], Loss: 1.3881, Train Accuracy: 63.88%\n",
      "Validation Accuracy: 81.72 %\n",
      "Epoch 152 completed. Current LR: 0.010324935136997806\n",
      "Epoch [153/300], Loss: 1.3777, Train Accuracy: 64.12%\n",
      "Validation Accuracy: 82.51 %\n",
      "Epoch 153 completed. Current LR: 0.010216644611751976\n",
      "Epoch [154/300], Loss: 1.3790, Train Accuracy: 64.12%\n",
      "Validation Accuracy: 82.67 %\n",
      "Epoch 154 completed. Current LR: 0.010108328662300398\n",
      "Epoch [155/300], Loss: 1.3602, Train Accuracy: 64.79%\n",
      "Validation Accuracy: 80.89 %\n",
      "Checkpoint saved at checkpoints/model_epoch_155.pth\n",
      "Epoch 155 completed. Current LR: 0.01\n",
      "Epoch [156/300], Loss: 1.3692, Train Accuracy: 64.63%\n",
      "Validation Accuracy: 82.21 %\n",
      "Epoch 156 completed. Current LR: 0.009891671337699602\n",
      "Epoch [157/300], Loss: 1.3560, Train Accuracy: 65.08%\n",
      "Validation Accuracy: 82.00 %\n",
      "Epoch 157 completed. Current LR: 0.009783355388248026\n",
      "Epoch [158/300], Loss: 1.3842, Train Accuracy: 63.76%\n",
      "Validation Accuracy: 81.81 %\n",
      "Epoch 158 completed. Current LR: 0.009675064863002196\n",
      "Epoch [159/300], Loss: 1.3563, Train Accuracy: 65.31%\n",
      "Validation Accuracy: 81.19 %\n",
      "Epoch 159 completed. Current LR: 0.00956681247033538\n",
      "Epoch [160/300], Loss: 1.3596, Train Accuracy: 64.92%\n",
      "Validation Accuracy: 82.15 %\n",
      "Checkpoint saved at checkpoints/model_epoch_160.pth\n",
      "Epoch 160 completed. Current LR: 0.009458610914145826\n",
      "Epoch [161/300], Loss: 1.3750, Train Accuracy: 64.17%\n",
      "Validation Accuracy: 82.32 %\n",
      "Epoch 161 completed. Current LR: 0.009350472892365893\n",
      "Epoch [162/300], Loss: 1.3699, Train Accuracy: 64.26%\n",
      "Validation Accuracy: 81.56 %\n",
      "Epoch 162 completed. Current LR: 0.009242411095471898\n",
      "Epoch [163/300], Loss: 1.3659, Train Accuracy: 64.63%\n",
      "Validation Accuracy: 80.63 %\n",
      "Epoch 163 completed. Current LR: 0.009134438204994827\n",
      "Epoch [164/300], Loss: 1.3571, Train Accuracy: 65.09%\n",
      "Validation Accuracy: 82.76 %\n",
      "Epoch 164 completed. Current LR: 0.009026566892032102\n",
      "Epoch [165/300], Loss: 1.3605, Train Accuracy: 65.04%\n",
      "Validation Accuracy: 82.64 %\n",
      "Checkpoint saved at checkpoints/model_epoch_165.pth\n",
      "Epoch 165 completed. Current LR: 0.008918809815760582\n",
      "Epoch [166/300], Loss: 1.3722, Train Accuracy: 64.18%\n",
      "Validation Accuracy: 83.28 %\n",
      "Epoch 166 completed. Current LR: 0.008811179621950936\n",
      "Epoch [167/300], Loss: 1.3310, Train Accuracy: 66.41%\n",
      "Validation Accuracy: 82.57 %\n",
      "Epoch 167 completed. Current LR: 0.008703688941483614\n",
      "Epoch [168/300], Loss: 1.3457, Train Accuracy: 65.41%\n",
      "Validation Accuracy: 83.72 %\n",
      "Epoch 168 completed. Current LR: 0.008596350388866558\n",
      "Epoch [169/300], Loss: 1.3476, Train Accuracy: 65.27%\n",
      "Validation Accuracy: 82.49 %\n",
      "Epoch 169 completed. Current LR: 0.008489176560754834\n",
      "Epoch [170/300], Loss: 1.3525, Train Accuracy: 65.32%\n",
      "Validation Accuracy: 83.79 %\n",
      "Checkpoint saved at checkpoints/model_epoch_170.pth\n",
      "Epoch 170 completed. Current LR: 0.008382180034472353\n",
      "Epoch [171/300], Loss: 1.3600, Train Accuracy: 64.81%\n",
      "Validation Accuracy: 81.87 %\n",
      "Epoch 171 completed. Current LR: 0.008275373366535877\n",
      "Epoch [172/300], Loss: 1.3582, Train Accuracy: 65.22%\n",
      "Validation Accuracy: 83.84 %\n",
      "Epoch 172 completed. Current LR: 0.00816876909118144\n",
      "Epoch [173/300], Loss: 1.3523, Train Accuracy: 65.39%\n",
      "Validation Accuracy: 83.37 %\n",
      "Epoch 173 completed. Current LR: 0.008062379718893418\n",
      "Epoch [174/300], Loss: 1.3834, Train Accuracy: 64.00%\n",
      "Validation Accuracy: 82.73 %\n",
      "Epoch 174 completed. Current LR: 0.007956217734936353\n",
      "Epoch [175/300], Loss: 1.3625, Train Accuracy: 64.56%\n",
      "Validation Accuracy: 83.68 %\n",
      "Checkpoint saved at checkpoints/model_epoch_175.pth\n",
      "Epoch 175 completed. Current LR: 0.007850295597889762\n",
      "Epoch [176/300], Loss: 1.3440, Train Accuracy: 65.52%\n",
      "Validation Accuracy: 81.11 %\n",
      "Epoch 176 completed. Current LR: 0.007744625738186056\n",
      "Epoch [177/300], Loss: 1.3819, Train Accuracy: 63.96%\n",
      "Validation Accuracy: 84.25 %\n",
      "Epoch 177 completed. Current LR: 0.007639220556651799\n",
      "Epoch [178/300], Loss: 1.3627, Train Accuracy: 64.89%\n",
      "Validation Accuracy: 83.03 %\n",
      "Epoch 178 completed. Current LR: 0.007534092423052381\n",
      "Epoch [179/300], Loss: 1.3242, Train Accuracy: 66.51%\n",
      "Validation Accuracy: 83.11 %\n",
      "Epoch 179 completed. Current LR: 0.007429253674640392\n",
      "Epoch [180/300], Loss: 1.3452, Train Accuracy: 65.40%\n",
      "Validation Accuracy: 84.60 %\n",
      "Checkpoint saved at checkpoints/model_epoch_180.pth\n",
      "Epoch 180 completed. Current LR: 0.007324716614707792\n",
      "Epoch [181/300], Loss: 1.3287, Train Accuracy: 66.37%\n",
      "Validation Accuracy: 83.99 %\n",
      "Epoch 181 completed. Current LR: 0.007220493511142058\n",
      "Epoch [182/300], Loss: 1.3428, Train Accuracy: 65.78%\n",
      "Validation Accuracy: 84.35 %\n",
      "Epoch 182 completed. Current LR: 0.0071165965949864934\n",
      "Epoch [183/300], Loss: 1.3454, Train Accuracy: 65.43%\n",
      "Validation Accuracy: 83.43 %\n",
      "Epoch 183 completed. Current LR: 0.007013038059004866\n",
      "Epoch [184/300], Loss: 1.3206, Train Accuracy: 66.53%\n",
      "Validation Accuracy: 84.89 %\n",
      "Epoch 184 completed. Current LR: 0.006909830056250529\n",
      "Epoch [185/300], Loss: 1.3407, Train Accuracy: 65.88%\n",
      "Validation Accuracy: 83.84 %\n",
      "Checkpoint saved at checkpoints/model_epoch_185.pth\n",
      "Epoch 185 completed. Current LR: 0.0068069846986402015\n",
      "Epoch [186/300], Loss: 1.3561, Train Accuracy: 65.16%\n",
      "Validation Accuracy: 83.85 %\n",
      "Epoch 186 completed. Current LR: 0.006704514055532599\n",
      "Epoch [187/300], Loss: 1.3438, Train Accuracy: 65.62%\n",
      "Validation Accuracy: 83.88 %\n",
      "Epoch 187 completed. Current LR: 0.006602430152312015\n",
      "Epoch [188/300], Loss: 1.3149, Train Accuracy: 66.93%\n",
      "Validation Accuracy: 84.12 %\n",
      "Epoch 188 completed. Current LR: 0.006500744968977116\n",
      "Epoch [189/300], Loss: 1.3611, Train Accuracy: 64.91%\n",
      "Validation Accuracy: 83.59 %\n",
      "Epoch 189 completed. Current LR: 0.006399470438735014\n",
      "Epoch [190/300], Loss: 1.3207, Train Accuracy: 66.52%\n",
      "Validation Accuracy: 84.23 %\n",
      "Checkpoint saved at checkpoints/model_epoch_190.pth\n",
      "Epoch 190 completed. Current LR: 0.006298618446600858\n",
      "Epoch [191/300], Loss: 1.3264, Train Accuracy: 66.34%\n",
      "Validation Accuracy: 83.37 %\n",
      "Epoch 191 completed. Current LR: 0.00619820082800308\n",
      "Epoch [192/300], Loss: 1.3123, Train Accuracy: 66.91%\n",
      "Validation Accuracy: 84.75 %\n",
      "Epoch 192 completed. Current LR: 0.006098229367394457\n",
      "Epoch [193/300], Loss: 1.2996, Train Accuracy: 67.67%\n",
      "Validation Accuracy: 83.76 %\n",
      "Epoch 193 completed. Current LR: 0.005998715796869137\n",
      "Epoch [194/300], Loss: 1.3252, Train Accuracy: 66.65%\n",
      "Validation Accuracy: 83.51 %\n",
      "Epoch 194 completed. Current LR: 0.005899671794785842\n",
      "Epoch [195/300], Loss: 1.3212, Train Accuracy: 66.45%\n",
      "Validation Accuracy: 84.56 %\n",
      "Checkpoint saved at checkpoints/model_epoch_195.pth\n",
      "Epoch 195 completed. Current LR: 0.005801108984397357\n",
      "Epoch [196/300], Loss: 1.3376, Train Accuracy: 65.67%\n",
      "Validation Accuracy: 84.24 %\n",
      "Epoch 196 completed. Current LR: 0.0057030389324864835\n",
      "Epoch [197/300], Loss: 1.3019, Train Accuracy: 67.46%\n",
      "Validation Accuracy: 84.92 %\n",
      "Epoch 197 completed. Current LR: 0.005605473148008637\n",
      "Epoch [198/300], Loss: 1.3166, Train Accuracy: 66.68%\n",
      "Validation Accuracy: 85.51 %\n",
      "Epoch 198 completed. Current LR: 0.00550842308074121\n",
      "Epoch [199/300], Loss: 1.3035, Train Accuracy: 67.37%\n",
      "Validation Accuracy: 85.48 %\n",
      "Epoch 199 completed. Current LR: 0.005411900119939892\n",
      "Epoch [200/300], Loss: 1.3119, Train Accuracy: 67.42%\n",
      "Validation Accuracy: 84.23 %\n",
      "Checkpoint saved at checkpoints/model_epoch_200.pth\n",
      "Epoch 200 completed. Current LR: 0.0053159155930021\n",
      "Epoch [201/300], Loss: 1.3392, Train Accuracy: 65.78%\n",
      "Validation Accuracy: 84.83 %\n",
      "Epoch 201 completed. Current LR: 0.005220480764137635\n",
      "Epoch [202/300], Loss: 1.3105, Train Accuracy: 67.00%\n",
      "Validation Accuracy: 85.29 %\n",
      "Epoch 202 completed. Current LR: 0.00512560683304681\n",
      "Epoch [203/300], Loss: 1.3119, Train Accuracy: 67.00%\n",
      "Validation Accuracy: 86.13 %\n",
      "Epoch 203 completed. Current LR: 0.00503130493360608\n",
      "Epoch [204/300], Loss: 1.3007, Train Accuracy: 67.27%\n",
      "Validation Accuracy: 85.93 %\n",
      "Epoch 204 completed. Current LR: 0.00493758613256146\n",
      "Epoch [205/300], Loss: 1.2922, Train Accuracy: 67.80%\n",
      "Validation Accuracy: 85.91 %\n",
      "Checkpoint saved at checkpoints/model_epoch_205.pth\n",
      "Epoch 205 completed. Current LR: 0.004844461428229785\n",
      "Epoch [206/300], Loss: 1.3087, Train Accuracy: 67.12%\n",
      "Validation Accuracy: 85.57 %\n",
      "Epoch 206 completed. Current LR: 0.004751941749207995\n",
      "Epoch [207/300], Loss: 1.3069, Train Accuracy: 67.15%\n",
      "Validation Accuracy: 86.45 %\n",
      "Epoch 207 completed. Current LR: 0.004660037953090643\n",
      "Epoch [208/300], Loss: 1.2907, Train Accuracy: 68.13%\n",
      "Validation Accuracy: 85.43 %\n",
      "Epoch 208 completed. Current LR: 0.004568760825195671\n",
      "Epoch [209/300], Loss: 1.2838, Train Accuracy: 68.50%\n",
      "Validation Accuracy: 85.08 %\n",
      "Epoch 209 completed. Current LR: 0.0044781210772987514\n",
      "Epoch [210/300], Loss: 1.3100, Train Accuracy: 67.19%\n",
      "Validation Accuracy: 86.32 %\n",
      "Checkpoint saved at checkpoints/model_epoch_210.pth\n",
      "Epoch 210 completed. Current LR: 0.004388129346376177\n",
      "Epoch [211/300], Loss: 1.2953, Train Accuracy: 67.69%\n",
      "Validation Accuracy: 85.51 %\n",
      "Epoch 211 completed. Current LR: 0.004298796193356566\n",
      "Epoch [212/300], Loss: 1.2900, Train Accuracy: 67.93%\n",
      "Validation Accuracy: 85.48 %\n",
      "Epoch 212 completed. Current LR: 0.0042101321018815155\n",
      "Epoch [213/300], Loss: 1.3161, Train Accuracy: 66.57%\n",
      "Validation Accuracy: 86.49 %\n",
      "Epoch 213 completed. Current LR: 0.0041221474770752695\n",
      "Epoch [214/300], Loss: 1.3013, Train Accuracy: 67.63%\n",
      "Validation Accuracy: 85.96 %\n",
      "Epoch 214 completed. Current LR: 0.004034852644323656\n",
      "Epoch [215/300], Loss: 1.2904, Train Accuracy: 68.18%\n",
      "Validation Accuracy: 86.09 %\n",
      "Checkpoint saved at checkpoints/model_epoch_215.pth\n",
      "Epoch 215 completed. Current LR: 0.003948257848062351\n",
      "Epoch [216/300], Loss: 1.3112, Train Accuracy: 66.99%\n",
      "Validation Accuracy: 86.28 %\n",
      "Epoch 216 completed. Current LR: 0.003862373250574626\n",
      "Epoch [217/300], Loss: 1.2738, Train Accuracy: 68.64%\n",
      "Validation Accuracy: 85.76 %\n",
      "Epoch 217 completed. Current LR: 0.0037772089307987976\n",
      "Epoch [218/300], Loss: 1.2859, Train Accuracy: 68.14%\n",
      "Validation Accuracy: 86.57 %\n",
      "Epoch 218 completed. Current LR: 0.00369277488314538\n",
      "Epoch [219/300], Loss: 1.3029, Train Accuracy: 67.04%\n",
      "Validation Accuracy: 86.25 %\n",
      "Epoch 219 completed. Current LR: 0.003609081016324243\n",
      "Epoch [220/300], Loss: 1.2849, Train Accuracy: 68.28%\n",
      "Validation Accuracy: 87.05 %\n",
      "Checkpoint saved at checkpoints/model_epoch_220.pth\n",
      "Epoch 220 completed. Current LR: 0.003526137152181724\n",
      "Epoch [221/300], Loss: 1.2851, Train Accuracy: 68.14%\n",
      "Validation Accuracy: 86.89 %\n",
      "Epoch 221 completed. Current LR: 0.00344395302454804\n",
      "Epoch [222/300], Loss: 1.2737, Train Accuracy: 68.60%\n",
      "Validation Accuracy: 85.97 %\n",
      "Epoch 222 completed. Current LR: 0.0033625382780949576\n",
      "Epoch [223/300], Loss: 1.2941, Train Accuracy: 67.73%\n",
      "Validation Accuracy: 86.31 %\n",
      "Epoch 223 completed. Current LR: 0.0032819024672039398\n",
      "Epoch [224/300], Loss: 1.3005, Train Accuracy: 67.56%\n",
      "Validation Accuracy: 85.36 %\n",
      "Epoch 224 completed. Current LR: 0.0032020550548449212\n",
      "Epoch [225/300], Loss: 1.2570, Train Accuracy: 69.62%\n",
      "Validation Accuracy: 87.01 %\n",
      "Checkpoint saved at checkpoints/model_epoch_225.pth\n",
      "Epoch 225 completed. Current LR: 0.0031230054114657693\n",
      "Epoch [226/300], Loss: 1.2843, Train Accuracy: 68.18%\n",
      "Validation Accuracy: 86.85 %\n",
      "Epoch 226 completed. Current LR: 0.0030447628138926155\n",
      "Epoch [227/300], Loss: 1.2827, Train Accuracy: 68.42%\n",
      "Validation Accuracy: 87.39 %\n",
      "Epoch 227 completed. Current LR: 0.0029673364442412034\n",
      "Epoch [228/300], Loss: 1.2619, Train Accuracy: 69.17%\n",
      "Validation Accuracy: 87.48 %\n",
      "Epoch 228 completed. Current LR: 0.002890735388839295\n",
      "Epoch [229/300], Loss: 1.2684, Train Accuracy: 68.92%\n",
      "Validation Accuracy: 87.21 %\n",
      "Epoch 229 completed. Current LR: 0.0028149686371603767\n",
      "Epoch [230/300], Loss: 1.2545, Train Accuracy: 69.49%\n",
      "Validation Accuracy: 88.35 %\n",
      "Checkpoint saved at checkpoints/model_epoch_230.pth\n",
      "Epoch 230 completed. Current LR: 0.0027400450807686916\n",
      "Epoch [231/300], Loss: 1.2472, Train Accuracy: 69.98%\n",
      "Validation Accuracy: 87.12 %\n",
      "Epoch 231 completed. Current LR: 0.0026659735122757746\n",
      "Epoch [232/300], Loss: 1.2559, Train Accuracy: 69.49%\n",
      "Validation Accuracy: 87.25 %\n",
      "Epoch 232 completed. Current LR: 0.0025927626243086098\n",
      "Epoch [233/300], Loss: 1.2369, Train Accuracy: 70.11%\n",
      "Validation Accuracy: 87.77 %\n",
      "Epoch 233 completed. Current LR: 0.002520421008489494\n",
      "Epoch [234/300], Loss: 1.2658, Train Accuracy: 68.94%\n",
      "Validation Accuracy: 88.01 %\n",
      "Epoch 234 completed. Current LR: 0.0024489571544277946\n",
      "Epoch [235/300], Loss: 1.2551, Train Accuracy: 69.39%\n",
      "Validation Accuracy: 88.28 %\n",
      "Checkpoint saved at checkpoints/model_epoch_235.pth\n",
      "Epoch 235 completed. Current LR: 0.0023783794487236367\n",
      "Epoch [236/300], Loss: 1.2360, Train Accuracy: 70.16%\n",
      "Validation Accuracy: 87.88 %\n",
      "Epoch 236 completed. Current LR: 0.002308696173983711\n",
      "Epoch [237/300], Loss: 1.2366, Train Accuracy: 70.10%\n",
      "Validation Accuracy: 86.93 %\n",
      "Epoch 237 completed. Current LR: 0.002239915507849273\n",
      "Epoch [238/300], Loss: 1.2257, Train Accuracy: 70.88%\n",
      "Validation Accuracy: 87.92 %\n",
      "Epoch 238 completed. Current LR: 0.0021720455220364445\n",
      "Epoch [239/300], Loss: 1.2374, Train Accuracy: 70.28%\n",
      "Validation Accuracy: 88.48 %\n",
      "Epoch 239 completed. Current LR: 0.0021050941813889833\n",
      "Epoch [240/300], Loss: 1.2466, Train Accuracy: 69.77%\n",
      "Validation Accuracy: 88.21 %\n",
      "Checkpoint saved at checkpoints/model_epoch_240.pth\n",
      "Epoch 240 completed. Current LR: 0.0020390693429435626\n",
      "Epoch [241/300], Loss: 1.2542, Train Accuracy: 69.37%\n",
      "Validation Accuracy: 88.96 %\n",
      "Epoch 241 completed. Current LR: 0.0019739787550077117\n",
      "Epoch [242/300], Loss: 1.2247, Train Accuracy: 70.89%\n",
      "Validation Accuracy: 87.99 %\n",
      "Epoch 242 completed. Current LR: 0.0019098300562505265\n",
      "Epoch [243/300], Loss: 1.2572, Train Accuracy: 69.47%\n",
      "Validation Accuracy: 88.67 %\n",
      "Epoch 243 completed. Current LR: 0.0018466307748062206\n",
      "Epoch [244/300], Loss: 1.2177, Train Accuracy: 71.13%\n",
      "Validation Accuracy: 88.73 %\n",
      "Epoch 244 completed. Current LR: 0.0017843883273906869\n",
      "Epoch [245/300], Loss: 1.2276, Train Accuracy: 70.60%\n",
      "Validation Accuracy: 88.25 %\n",
      "Checkpoint saved at checkpoints/model_epoch_245.pth\n",
      "Epoch 245 completed. Current LR: 0.0017231100184310955\n",
      "Epoch [246/300], Loss: 1.2173, Train Accuracy: 71.25%\n",
      "Validation Accuracy: 88.92 %\n",
      "Epoch 246 completed. Current LR: 0.001662803039208698\n",
      "Epoch [247/300], Loss: 1.2594, Train Accuracy: 69.30%\n",
      "Validation Accuracy: 88.57 %\n",
      "Epoch 247 completed. Current LR: 0.0016034744670148972\n",
      "Epoch [248/300], Loss: 1.2351, Train Accuracy: 70.43%\n",
      "Validation Accuracy: 88.87 %\n",
      "Epoch 248 completed. Current LR: 0.0015451312643206827\n",
      "Epoch [249/300], Loss: 1.2323, Train Accuracy: 70.44%\n",
      "Validation Accuracy: 89.00 %\n",
      "Epoch 249 completed. Current LR: 0.0014877802779595783\n",
      "Epoch [250/300], Loss: 1.2321, Train Accuracy: 70.26%\n",
      "Validation Accuracy: 89.00 %\n",
      "Checkpoint saved at checkpoints/model_epoch_250.pth\n",
      "Epoch 250 completed. Current LR: 0.0014314282383241096\n",
      "Epoch [251/300], Loss: 1.2185, Train Accuracy: 70.94%\n",
      "Validation Accuracy: 89.39 %\n",
      "Epoch 251 completed. Current LR: 0.0013760817585759789\n",
      "Epoch [252/300], Loss: 1.2098, Train Accuracy: 71.34%\n",
      "Validation Accuracy: 89.11 %\n",
      "Epoch 252 completed. Current LR: 0.001321747333869986\n",
      "Epoch [253/300], Loss: 1.2201, Train Accuracy: 71.01%\n",
      "Validation Accuracy: 89.08 %\n",
      "Epoch 253 completed. Current LR: 0.0012684313405917703\n",
      "Epoch [254/300], Loss: 1.1973, Train Accuracy: 72.00%\n",
      "Validation Accuracy: 89.41 %\n",
      "Epoch 254 completed. Current LR: 0.0012161400356095376\n",
      "Epoch [255/300], Loss: 1.2218, Train Accuracy: 70.72%\n",
      "Validation Accuracy: 89.51 %\n",
      "Checkpoint saved at checkpoints/model_epoch_255.pth\n",
      "Epoch 255 completed. Current LR: 0.0011648795555397717\n",
      "Epoch [256/300], Loss: 1.2198, Train Accuracy: 70.74%\n",
      "Validation Accuracy: 89.57 %\n",
      "Epoch 256 completed. Current LR: 0.0011146559160270875\n",
      "Epoch [257/300], Loss: 1.1995, Train Accuracy: 71.90%\n",
      "Validation Accuracy: 89.73 %\n",
      "Epoch 257 completed. Current LR: 0.001065475011038265\n",
      "Epoch [258/300], Loss: 1.2115, Train Accuracy: 71.29%\n",
      "Validation Accuracy: 89.32 %\n",
      "Epoch 258 completed. Current LR: 0.0010173426121705576\n",
      "Epoch [259/300], Loss: 1.1912, Train Accuracy: 72.12%\n",
      "Validation Accuracy: 89.83 %\n",
      "Epoch 259 completed. Current LR: 0.0009702643679743839\n",
      "Epoch [260/300], Loss: 1.1864, Train Accuracy: 72.26%\n",
      "Validation Accuracy: 89.67 %\n",
      "Checkpoint saved at checkpoints/model_epoch_260.pth\n",
      "Epoch 260 completed. Current LR: 0.0009242458032904311\n",
      "Epoch [261/300], Loss: 1.2294, Train Accuracy: 70.45%\n",
      "Validation Accuracy: 90.05 %\n",
      "Epoch 261 completed. Current LR: 0.0008792923186013002\n",
      "Epoch [262/300], Loss: 1.1900, Train Accuracy: 72.34%\n",
      "Validation Accuracy: 89.80 %\n",
      "Epoch 262 completed. Current LR: 0.0008354091893977401\n",
      "Epoch [263/300], Loss: 1.2179, Train Accuracy: 70.83%\n",
      "Validation Accuracy: 90.09 %\n",
      "Epoch 263 completed. Current LR: 0.0007926015655595242\n",
      "Epoch [264/300], Loss: 1.1998, Train Accuracy: 71.72%\n",
      "Validation Accuracy: 89.63 %\n",
      "Epoch 264 completed. Current LR: 0.0007508744707511117\n",
      "Epoch [265/300], Loss: 1.1906, Train Accuracy: 72.41%\n",
      "Validation Accuracy: 90.09 %\n",
      "Checkpoint saved at checkpoints/model_epoch_265.pth\n",
      "Epoch 265 completed. Current LR: 0.0007102328018320858\n",
      "Epoch [266/300], Loss: 1.2095, Train Accuracy: 71.26%\n",
      "Validation Accuracy: 89.93 %\n",
      "Epoch 266 completed. Current LR: 0.0006706813282824897\n",
      "Epoch [267/300], Loss: 1.1798, Train Accuracy: 72.43%\n",
      "Validation Accuracy: 90.49 %\n",
      "Epoch 267 completed. Current LR: 0.0006322246916431106\n",
      "Epoch [268/300], Loss: 1.1867, Train Accuracy: 72.23%\n",
      "Validation Accuracy: 89.88 %\n",
      "Epoch 268 completed. Current LR: 0.0005948674049707603\n",
      "Epoch [269/300], Loss: 1.1956, Train Accuracy: 72.23%\n",
      "Validation Accuracy: 90.24 %\n",
      "Epoch 269 completed. Current LR: 0.0005586138523086681\n",
      "Epoch [270/300], Loss: 1.1735, Train Accuracy: 73.10%\n",
      "Validation Accuracy: 90.09 %\n",
      "Checkpoint saved at checkpoints/model_epoch_270.pth\n",
      "Epoch 270 completed. Current LR: 0.0005234682881719765\n",
      "Epoch [271/300], Loss: 1.1796, Train Accuracy: 72.51%\n",
      "Validation Accuracy: 90.72 %\n",
      "Epoch 271 completed. Current LR: 0.0004894348370484647\n",
      "Epoch [272/300], Loss: 1.1913, Train Accuracy: 72.17%\n",
      "Validation Accuracy: 90.43 %\n",
      "Epoch 272 completed. Current LR: 0.0004565174929145188\n",
      "Epoch [273/300], Loss: 1.1729, Train Accuracy: 72.89%\n",
      "Validation Accuracy: 89.97 %\n",
      "Epoch 273 completed. Current LR: 0.0004247201187664218\n",
      "Epoch [274/300], Loss: 1.2012, Train Accuracy: 71.61%\n",
      "Validation Accuracy: 90.35 %\n",
      "Epoch 274 completed. Current LR: 0.00039404644616701344\n",
      "Epoch [275/300], Loss: 1.1566, Train Accuracy: 73.66%\n",
      "Validation Accuracy: 90.07 %\n",
      "Checkpoint saved at checkpoints/model_epoch_275.pth\n",
      "Epoch 275 completed. Current LR: 0.0003645000748077709\n",
      "Epoch [276/300], Loss: 1.1905, Train Accuracy: 72.29%\n",
      "Validation Accuracy: 90.61 %\n",
      "Epoch 276 completed. Current LR: 0.00033608447208637537\n",
      "Epoch [277/300], Loss: 1.1708, Train Accuracy: 72.93%\n",
      "Validation Accuracy: 90.47 %\n",
      "Epoch 277 completed. Current LR: 0.0003088029726997965\n",
      "Epoch [278/300], Loss: 1.1547, Train Accuracy: 73.54%\n",
      "Validation Accuracy: 90.35 %\n",
      "Epoch 278 completed. Current LR: 0.0002826587782529444\n",
      "Epoch [279/300], Loss: 1.1850, Train Accuracy: 72.33%\n",
      "Validation Accuracy: 90.99 %\n",
      "Epoch 279 completed. Current LR: 0.0002576549568829578\n",
      "Epoch [280/300], Loss: 1.1606, Train Accuracy: 73.35%\n",
      "Validation Accuracy: 91.04 %\n",
      "Checkpoint saved at checkpoints/model_epoch_280.pth\n",
      "Epoch 280 completed. Current LR: 0.00023379444289913344\n",
      "Epoch [281/300], Loss: 1.1570, Train Accuracy: 73.73%\n",
      "Validation Accuracy: 91.05 %\n",
      "Epoch 281 completed. Current LR: 0.0002110800364385812\n",
      "Epoch [282/300], Loss: 1.1625, Train Accuracy: 73.50%\n",
      "Validation Accuracy: 90.40 %\n",
      "Epoch 282 completed. Current LR: 0.00018951440313760837\n",
      "Epoch [283/300], Loss: 1.1804, Train Accuracy: 72.61%\n",
      "Validation Accuracy: 90.81 %\n",
      "Epoch 283 completed. Current LR: 0.0001691000738188997\n",
      "Epoch [284/300], Loss: 1.1745, Train Accuracy: 72.80%\n",
      "Validation Accuracy: 90.75 %\n",
      "Epoch 284 completed. Current LR: 0.00014983944419451613\n",
      "Epoch [285/300], Loss: 1.1658, Train Accuracy: 73.19%\n",
      "Validation Accuracy: 90.73 %\n",
      "Checkpoint saved at checkpoints/model_epoch_285.pth\n",
      "Epoch 285 completed. Current LR: 0.0001317347745847386\n",
      "Epoch [286/300], Loss: 1.1592, Train Accuracy: 73.50%\n",
      "Validation Accuracy: 90.84 %\n",
      "Epoch 286 completed. Current LR: 0.00011478818965281912\n",
      "Epoch [287/300], Loss: 1.1735, Train Accuracy: 72.86%\n",
      "Validation Accuracy: 90.40 %\n",
      "Epoch 287 completed. Current LR: 9.900167815563466e-05\n",
      "Epoch [288/300], Loss: 1.1799, Train Accuracy: 72.25%\n",
      "Validation Accuracy: 90.75 %\n",
      "Epoch 288 completed. Current LR: 8.437709271030602e-05\n",
      "Epoch [289/300], Loss: 1.1617, Train Accuracy: 73.44%\n",
      "Validation Accuracy: 90.92 %\n",
      "Epoch 289 completed. Current LR: 7.091614957677628e-05\n",
      "Epoch [290/300], Loss: 1.1991, Train Accuracy: 71.83%\n",
      "Validation Accuracy: 90.69 %\n",
      "Checkpoint saved at checkpoints/model_epoch_290.pth\n",
      "Epoch 290 completed. Current LR: 5.862042845640403e-05\n",
      "Epoch [291/300], Loss: 1.1614, Train Accuracy: 73.39%\n",
      "Validation Accuracy: 90.75 %\n",
      "Epoch 291 completed. Current LR: 4.749137230658063e-05\n",
      "Epoch [292/300], Loss: 1.1546, Train Accuracy: 73.76%\n",
      "Validation Accuracy: 91.17 %\n",
      "Epoch 292 completed. Current LR: 3.753028717138785e-05\n",
      "Epoch [293/300], Loss: 1.1457, Train Accuracy: 74.08%\n",
      "Validation Accuracy: 91.27 %\n",
      "Epoch 293 completed. Current LR: 2.873834202833159e-05\n",
      "Epoch [294/300], Loss: 1.1526, Train Accuracy: 73.65%\n",
      "Validation Accuracy: 91.01 %\n",
      "Epoch 294 completed. Current LR: 2.1116568651156076e-05\n",
      "Epoch [295/300], Loss: 1.1799, Train Accuracy: 72.72%\n",
      "Validation Accuracy: 90.91 %\n",
      "Checkpoint saved at checkpoints/model_epoch_295.pth\n",
      "Epoch 295 completed. Current LR: 1.4665861488761812e-05\n",
      "Epoch [296/300], Loss: 1.1789, Train Accuracy: 72.65%\n",
      "Validation Accuracy: 90.69 %\n",
      "Epoch 296 completed. Current LR: 9.386977560232879e-06\n",
      "Epoch [297/300], Loss: 1.1475, Train Accuracy: 73.93%\n",
      "Validation Accuracy: 90.87 %\n",
      "Epoch 297 completed. Current LR: 5.280536366004674e-06\n",
      "Epoch [298/300], Loss: 1.1710, Train Accuracy: 72.91%\n",
      "Validation Accuracy: 90.59 %\n",
      "Epoch 298 completed. Current LR: 2.347019815158724e-06\n",
      "Epoch [299/300], Loss: 1.1537, Train Accuracy: 73.72%\n",
      "Validation Accuracy: 91.03 %\n",
      "Epoch 299 completed. Current LR: 5.867721688690431e-07\n",
      "Epoch [300/300], Loss: 1.1544, Train Accuracy: 73.41%\n",
      "Validation Accuracy: 90.69 %\n",
      "Checkpoint saved at checkpoints/model_epoch_300.pth\n",
      "Epoch 300 completed. Current LR: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming mixup_data and mixup_criterion functions are defined as shown earlier.\n",
    "\n",
    "total_step = len(train_loader)\n",
    "last = 0\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0       # Accumulate weighted correct predictions for training accuracy.\n",
    "    running_total = 0         # Accumulate total examples.\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device using non_blocking transfer\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # Apply mixup on the batch\n",
    "        mixed_images, targets_a, targets_b, lam = advanced_mixup_data(images, labels)\n",
    "        # Forward pass with the mixed images\n",
    "        outputs = model(mixed_images)\n",
    "        # Compute mixup loss\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # For mixup, computing \"accuracy\" is not straightforward.\n",
    "        # One approximation is to compute a weighted accuracy:\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_a = (predicted == targets_a).sum().item()\n",
    "        correct_b = (predicted == targets_b).sum().item()\n",
    "        # Weighted sum of correct predictions using mixup lambda.\n",
    "        running_correct += lam * correct_a + (1 - lam) * correct_b\n",
    "        running_total += labels.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log training loss for the batch (logging remains unchanged)\n",
    "        wandb.log({\"train/loss\": loss.item(), \"epoch\": epoch, \"batch\": i})\n",
    "\n",
    "    # Calculate average training loss and approximate training accuracy for the epoch.\n",
    "    avg_loss = running_loss / total_step\n",
    "    train_accuracy = 100 * running_correct / running_total\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Train Accuracy: {:.2f}%'.format(\n",
    "        epoch+1, num_epochs, avg_loss, train_accuracy))\n",
    "\n",
    "    # Validation after each epoch (use original images, no mixup during validation)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss_val = criterion(outputs, labels)\n",
    "            val_loss += loss_val.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print('Validation Accuracy: {:.2f} %'.format(val_accuracy))\n",
    "\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint_path = f\"{checkpoint_dir}/model_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    last = epoch + 1\n",
    "\n",
    "    # Step the scheduler at the end of each epoch to update the learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]  # Capture current learning rate.\n",
    "    print(f\"Epoch {epoch+1} completed. Current LR: {current_lr}\")\n",
    "\n",
    "    # Log aggregated metrics for the epoch to wandb.\n",
    "    wandb.log({\n",
    "        \"train/avg_loss\": avg_loss,\n",
    "        \"train/accuracy\": train_accuracy,\n",
    "        \"val/avg_loss\": avg_val_loss,\n",
    "        \"val/accuracy\": val_accuracy,\n",
    "        \"lr\": current_lr,\n",
    "        \"epoch\": epoch\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YG1meAaVQW2l",
    "outputId": "ac52a93e-8a70-4f90-ab51-711c2c728d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact 'resnet_experiment_20250313_180158' logged to wandb!\n"
     ]
    }
   ],
   "source": [
    "# save the model checkpoints to wandb\n",
    "\n",
    "# Find all checkpoint files that match the naming pattern\n",
    "checkpoint_files = glob.glob(f\"{name}_epoch_{last}.pth\")\n",
    "\n",
    "# Create a new artifact with the same name as the experiment\n",
    "artifact = wandb.Artifact(name, type=\"model\")\n",
    "\n",
    "# Add each checkpoint file to the artifact\n",
    "for cp in checkpoint_files:\n",
    "    artifact.add_file(cp)\n",
    "    print(f\"Added {cp} to the artifact.\")\n",
    "\n",
    "# Log the artifact to wandb\n",
    "wandb.log_artifact(artifact)\n",
    "print(f\"Artifact '{name}' logged to wandb!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTanFvTTfbAQ",
    "outputId": "8fe317a9-7721-48e4-c9b9-04138ec1c110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95.94 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "    test_accuracy = 100 * correct / total\n",
    "    wandb.log({\"test/accuracy\": test_accuracy})\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "E1UTTPp3WnhA",
    "outputId": "938ae6da-1021-4a1b-f525-c93638832b36"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▅▆█▅▂█▅▆▁▅▂▇▂▆▂▃▆▄▃▆▇▃▁▇█▁█▃▅▂▄█▅█▇▇▅▇▄▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇██</td></tr><tr><td>lr</td><td>▆█████████▇▇▇▇▇▇▇▆▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>train/accuracy</td><td>▁▄▄▅▅▅▅▅▆▅▅▆▅▆▆▆▆▆▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train/avg_loss</td><td>█▆▆▆▅▄▄▄▃▄▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▆▆▆▄█▇▅▇▄▆█▅▂█▄▄▂▅▅▄▆▂▆▅▆▆▁▁▂▂▅▆▄▆▃▂▁▆▄▆</td></tr><tr><td>val/accuracy</td><td>▁▃▄▄▅▆▅▆▆▆▆▆▇▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>val/avg_loss</td><td>█▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>664</td></tr><tr><td>epoch</td><td>299</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>test/accuracy</td><td>95.94</td></tr><tr><td>train/accuracy</td><td>73.4075</td></tr><tr><td>train/avg_loss</td><td>1.15436</td></tr><tr><td>train/loss</td><td>1.51346</td></tr><tr><td>val/accuracy</td><td>90.69333</td></tr><tr><td>val/avg_loss</td><td>0.78749</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet_experiment_20250313_180158</strong> at: <a href='https://wandb.ai/pb3073-new-york-university/DL_project1/runs/8pm6w0c0' target=\"_blank\">https://wandb.ai/pb3073-new-york-university/DL_project1/runs/8pm6w0c0</a><br> View project at: <a href='https://wandb.ai/pb3073-new-york-university/DL_project1' target=\"_blank\">https://wandb.ai/pb3073-new-york-university/DL_project1</a><br>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250313_180158-8pm6w0c0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"resnet_model.pth\")\n",
    "wandb.save(\"resnet_model.pth\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OyHivkMzUbk"
   },
   "source": [
    "# Process Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Anst427vzV4M"
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00SEvgAQzWWs",
    "outputId": "c3b363dc-75e7-401f-c4ae-250ef4240ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (10000, 32, 32, 3)\n",
      "Data shape after permutation: torch.Size([10000, 3, 32, 32])\n",
      "Test data shape: torch.Size([10000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load the unpickled data\n",
    "data_unpickle = unpickle('./data/cifar_test_nolabel.pkl')\n",
    "data = data_unpickle[b'data']\n",
    "print(\"Original data shape:\", data.shape)  # Expect (N, 3072)\n",
    "\n",
    "# Reshape the data: (N, 3072) -> (N, 32, 32, 3)\n",
    "data = data.reshape(-1, 32, 32, 3).astype('float32') / 255.0\n",
    "\n",
    "# Convert numpy array to a torch tensor\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Permute dimensions: from (N, H, W, C) to (N, C, H, W)\n",
    "data = data.permute(0, 3, 1, 2)\n",
    "print(\"Data shape after permutation:\", data.shape)  # Expect (N, 3, 32, 32)\n",
    "\n",
    "# Calculate the mean and standard deviation from the test data\n",
    "# mean = data.mean(dim=(0, 2, 3), keepdim=True)\n",
    "# std = data.std(dim=(0, 2, 3), keepdim=True)\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465], dtype=torch.float32).view(3,1,1)\n",
    "std = torch.tensor([0.2023, 0.1994, 0.2010], dtype=torch.float32).view(3,1,1)\n",
    "\n",
    "# Apply normalization: (x - mean) / std for each channel\n",
    "data = (data - mean) / std\n",
    "\n",
    "# Move data to device (ensure that 'device' is defined)\n",
    "data = data.to(device)\n",
    "\n",
    "# Load label names\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\"Test data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGChzPlH1wUI",
    "outputId": "8b29c4bd-4c0f-4e03-f264-0e45932d7690"
   },
   "outputs": [],
   "source": [
    "# Run inference on custom test data\n",
    "predicted_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(data)):\n",
    "        image = data[i].unsqueeze(0)  # Add batch dimension\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.append(predicted.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels saved to resnet_experiment_20250313_180158.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predicted labels to a CSV file\n",
    "with open(f'./prediction_csv/{name}_orig_norm.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'Labels'])\n",
    "    for idx, label in enumerate(predicted_labels):\n",
    "        writer.writerow([idx, label])\n",
    "\n",
    "print(f\"Predicted labels saved to {name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwyel7mW2ZpM",
    "outputId": "0922134f-2ae9-4843-9c31-28e71794e67b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog(6): 918\n",
      "automobile(1): 1034\n",
      "ship(8): 1046\n",
      "truck(9): 1014\n",
      "cat(3): 1041\n",
      "airplane(0): 876\n",
      "bird(2): 979\n",
      "dog(5): 1110\n",
      "horse(7): 1010\n",
      "deer(4): 972\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(predicted_labels)\n",
    "\n",
    "# Print label counts\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label_names[label]}({label}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['ID', 'Labels']\n",
      "match percent: 0.7755\n"
     ]
    }
   ],
   "source": [
    "with open('reference_labels.csv', 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    header = next(csv_reader)\n",
    "    print(\"Header:\", header)\n",
    "    same=0\n",
    "    different=0\n",
    "    for idx, label in enumerate(predicted_labels):\n",
    "        r = next(csv_reader)\n",
    "        if int(r[1])==label:\n",
    "            same+=1\n",
    "        else:\n",
    "            different+=1\n",
    "    print(f\"match percent: {same/(same+different)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
