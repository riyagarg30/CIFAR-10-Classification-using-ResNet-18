# ðŸ§  CIFAR-10 Classification with Modified ResNet-18

A deep learning project using a custom **ResNet-18** with **Squeeze-and-Excitation (SE)** blocks, **Mixup & CutMix**, **dropout**, and **advanced augmentations** â€” trained and evaluated on CIFAR-10, achieving **95.94% test accuracy** under a strict 5M parameter limit.

> **Authors**: Pranav Bhatt, Kevin Mai, Riya Garg  
> ðŸ“„ [Read the Report](./report.pdf)

---

## ðŸ“¦ Files Included

| File / Folder               | Description                                                 |
|----------------------------|-------------------------------------------------------------|
| `resnet18_se_cifar10.ipynb`| Jupyter notebook with full training, validation, inference  |
| `report.pdf`               | Project write-up with architecture, results, and analysis   |
| `cifar_test_nolabel.pkl`   | Unlabeled CIFAR-10 test images (used in inference section)  |
| `prediction_csv/`          | Output CSV file with predicted labels                       |

---

## ðŸš€ Highlights

- âœ… **95.94% test accuracy** with <5M parameters
- ðŸ”§ ResNet-18 modified with `[3, 3, 4, 3]` block config
- ðŸ”„ **Squeeze-and-Excitation blocks** for channel-wise attention
- ðŸ”€ **Mixup & CutMix** augmentation
- ðŸ§ª Advanced data augmentation pipeline
- ðŸ”¥ **Warm-up + cosine LR scheduling**
- ðŸ§¼ Inference-ready for `.pkl` test file

---

## ðŸ§ª Setup & Run

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/dl_project_1_public.git
cd dl_project_1_public
````

### 2. Install Required Packages

```bash
pip install torch torchvision numpy matplotlib wandb torchsummary
```

### 3. Run the Notebook

```bash
jupyter notebook resnet18_se_cifar10.ipynb
```

> ðŸ”‘ Replace your `wandb.login(key=...)` key in the notebook if needed.

---

## ðŸ“Š Results

| Metric              | Value      |
| ------------------- | ---------- |
| âœ… Test Accuracy     | **95.94%** |
| Validation Accuracy | 90.69%     |
| Training Accuracy   | 73.41%     |

---

## ðŸ“¤ Inference Output

* Inference is performed on `cifar_test_nolabel.pkl`
* Predictions are saved to:
  `prediction_csv/resnet_experiment_YYYYMMDD_HHMMSS_orig_norm.csv`

---

## ðŸ”„ Data Augmentation Techniques

* âœ… Random crop, flip, rotation
* âœ… Color jitter, sharpness, AutoAugment
* âœ… Random erasing
* âœ… Normalization (ImageNet stats)
* âœ… Mixup & CutMix (50/50 hybrid)

---

## ðŸ§  Model Architecture

* Conv1: 3Ã—3, 32 filters
* Residual Blocks: `[3, 3, 4, 3]` layers
* SE Block: After convs in each block
* Dropout before final FC
* Loss: Label Smoothing Cross-Entropy
* Optimizer: SGD + Momentum + Weight Decay
* Scheduler: Warm-up (10 epochs) â†’ Cosine Annealing

---

## ðŸ“„ References

* He et al. (2015) â€” [Deep Residual Learning](https://arxiv.org/abs/1512.03385)
* Hu et al. (2017) â€” [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)
* Zhang et al. (2017) â€” [Mixup Augmentation](https://arxiv.org/abs/1710.09412)

